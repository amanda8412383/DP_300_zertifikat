{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1e27f3",
   "metadata": {},
   "source": [
    "# lesson 6 configure azure sql db for scale and performance\n",
    "\n",
    "**note:backup storage redundancy** \n",
    "\n",
    "geo redundant backup will replicate the storage to a paired region (e.g. US East to Us West) in case of data storage damaging\n",
    "\n",
    "**note:billing type** \n",
    "- provisional charged based on vcore\n",
    "- serverless charged based on second, shut down after 1 hr inactivity, restarted takes a little time\n",
    "\n",
    "\n",
    "**note:hybrid benefit** \n",
    "- it's a Microsoft licensing program that allows customers to use their existing on-premises Windows Server and SQL Server licenses with active Software Assurance or subscription to reduce costs when running those workloads in Azure\n",
    "- can only work with vcore, not with dtu\n",
    "**note:elastic pool** \n",
    "\n",
    "- let manage multiple db resource allocation possible, so they could share resource when one not using it\n",
    "- not possible for hyperscale\n",
    "- can select edtu (elastic dtu) in setting\n",
    "- unit price for edut compared to dtu is extra 50 %, vcore have same unti price\n",
    "- if a DTU db peak at different time, elastic pool could be a good idea, otherwise it's more expensive due to unit price\n",
    "- unavailable at hyperscale\n",
    "\n",
    "\n",
    "\n",
    "### storage purchasing model\n",
    "- can be changed later, except changed out of hyperscale\n",
    "- change should be done at a low trafic time\n",
    "- after change DMV(dynamic management view) needed to be flushed for accurate estimation\n",
    "exec sq_query_store_flush\n",
    "##### DTU (database transaction unit)based\n",
    "- DTU can be adjusted with related S tier\n",
    "- CDC (changed data captured) can't be used when having less than 100 DTB (= S3 = 1vcore)\n",
    "- when using more than 300 DTU, vcore might be a cheaper option\n",
    "- plan selection can based on peak usage and storage need, the latter could be estimate with average data use * db number \n",
    "1. Basic: up to 2GB\n",
    "Development, testing, infrequently accessed workload\n",
    "2. Standard\n",
    "3. Premium\n",
    "suitable when more iops (inputs output per second) is needed\n",
    "##### vCore (virtual core) based\n",
    "- max data storgae and core could be specified\n",
    "- core decide max data storgae ceiling & tempdb size\n",
    "- log space is 30% of the max data stoarge\n",
    "- hardware choice affect memory, core number and storage\n",
    "- iops = concurrent number of request, this can't be configured, but go up with vcore\n",
    "\n",
    "4. general purpose: 80 vcore, 4 terabyte storage\n",
    "suitable for low latency input\n",
    "5. Hyperscale: 80 vcore, 100 terabyte storage\n",
    "6. Business critical: 80 vcore, 4 terabyte storage, fast geo-recovery and advanced data corruption protection, free second read-only replica\n",
    "Business critical has high transaction rate and high resiliency, is suitable for large number/long-running transaction\n",
    "\n",
    "### table partitioning\n",
    "\n",
    "- a table have 3 elements storage space, computing resource limit (exceeing might lead to timeout) and network bandwidth limit\n",
    "- vertical scaling by adding disc space, processing power, memory and network connection could be a short term solution\n",
    "- horizontal scaling by divifing the table up according to rule e.g. by year, it add up complexity but reduce hardware demand\n",
    "- back up per partition is also faster \n",
    "- the other option is to split the table columns up\n",
    "\n",
    "### database  sharding\n",
    "- when we apply horizontal scaling on db level\n",
    "- there would be a table with shard key, and where they are stored, this require more setting up \n",
    "- range strategy is diving by range e.g.years, sequential shard, data might be unblanced when there is seasonality\n",
    "- hashing strategy introduce random element for distribution, it reduce hotspot bcs no cluster would be more used, but rebalancing and manging mmight be difficult, when looking up a value the results are all over places\n",
    "- functional partitioning, not putting all the tables in the same db, e.g. seperate tables that needs extra security\n",
    "- keeping data geographically close can reduce latency when using\n",
    "\n",
    "**note: file group**\n",
    "- can't be used in Azure SQL db (only primary file group there)\n",
    "- file can be putted in file group to define partition range rules and where to store\n",
    "\n",
    "### compression for table\n",
    "- pro reduced space\n",
    "- con takes extra power and time to compress and retrieve \n",
    "- can't be used on table with sparse columns\n",
    "- changing compression require droping clustered index\n",
    "- complete index viewed can be compressed\n",
    "- system table can't be compressed\n",
    "- table with different partition can be compressed differently\n",
    "- *exec sp_estimate_data_compression_savings* help estimate compression effect\n",
    "\n",
    "method:\n",
    "1. no compression\n",
    "2. row compression: char and nchar columns would be greatly reduced, varchar and int not so much, date types depends, tiny int not at all\n",
    "3. page compression which includes (below element can't be activated seperately):\n",
    "    1. row compression\n",
    "    2. prefix compression: store long duplicated value in a specific column as prefix key, and generate an extra table for prefix key to long name to prevent the duplication of long char storage\n",
    "    3. dictionary compression: similar to prefix compression, but instead of on a specific column, the duplication would be searched across columns and keys would be used for whole table\n",
    "\n",
    "    - page is a set of row up to 8192 character\n",
    "    - uncompressed at first, when additional rows can't be fit in, compression takes place\n",
    "    - some older version SQL server on VM is this unavailable\n",
    "\n",
    "**note heap**\n",
    "haep is table without clustered index\n",
    "\n",
    "**columnstore table**\n",
    "above are row stored table. in columnstore table, columns are always compress, which could be further compressed through columnstore archival compression (but this has super high compute cost for uncompress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db218",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
