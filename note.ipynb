{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1e27f3",
   "metadata": {},
   "source": [
    "# lesson 6 configure azure sql db for scale and performance\n",
    "\n",
    "**note:backup storage redundancy** \n",
    "\n",
    "geo redundant backup will replicate the storage to a paired region (e.g. US East to Us West) in case of data storage damaging\n",
    "\n",
    "**note:billing type** \n",
    "- provisional charged based on vcore\n",
    "- serverless charged based on second, shut down after 1 hr inactivity, restarted takes a little time\n",
    "\n",
    "\n",
    "**note:hybrid benefit** \n",
    "- it's a Microsoft licensing program that allows customers to use their existing on-premises Windows Server and SQL Server licenses with active Software Assurance or subscription to reduce costs when running those workloads in Azure\n",
    "- can only work with vcore, not with dtu\n",
    "**note:elastic pool** \n",
    "\n",
    "- let manage multiple db resource allocation possible, so they could share resource when one not using it\n",
    "- not possible for hyperscale\n",
    "- can select edtu (elastic dtu) in setting\n",
    "- unit price for edut compared to dtu is extra 50 %, vcore have same unti price\n",
    "- if a DTU db peak at different time, elastic pool could be a good idea, otherwise it's more expensive due to unit price\n",
    "- unavailable at hyperscale\n",
    "\n",
    "\n",
    "\n",
    "### storage purchasing model\n",
    "- can be changed later, except changed out of hyperscale\n",
    "- change should be done at a low trafic time\n",
    "- after change DMV(dynamic management view) needed to be flushed for accurate estimation\n",
    "exec sq_query_store_flush\n",
    "##### DTU (database transaction unit)based\n",
    "- DTU can be adjusted with related S tier\n",
    "- CDC (changed data captured) can't be used when having less than 100 DTB (= S3 = 1vcore)\n",
    "- when using more than 300 DTU, vcore might be a cheaper option\n",
    "- plan selection can based on peak usage and storage need, the latter could be estimate with average data use * db number \n",
    "1. Basic: up to 2GB\n",
    "Development, testing, infrequently accessed workload\n",
    "2. Standard\n",
    "3. Premium\n",
    "suitable when more iops (inputs output per second) is needed\n",
    "##### vCore (virtual core) based\n",
    "- max data storgae and core could be specified\n",
    "- core decide max data storgae ceiling & tempdb size\n",
    "- log space is 30% of the max data stoarge\n",
    "- hardware choice affect memory, core number and storage\n",
    "- iops = concurrent number of request, this can't be configured, but go up with vcore\n",
    "\n",
    "4. general purpose: 80 vcore, 4 terabyte storage\n",
    "suitable for low latency input\n",
    "5. Hyperscale: 80 vcore, 100 terabyte storage\n",
    "6. Business critical: 80 vcore, 4 terabyte storage, fast geo-recovery and advanced data corruption protection, free second read-only replica\n",
    "Business critical has high transaction rate and high resiliency, is suitable for large number/long-running transaction\n",
    "\n",
    "### table partitioning\n",
    "\n",
    "- a table have 3 elements storage space, computing resource limit (exceeing might lead to timeout) and network bandwidth limit\n",
    "- vertical scaling by adding disc space, processing power, memory and network connection could be a short term solution\n",
    "- horizontal scaling by divifing the table up according to rule e.g. by year, it add up complexity but reduce hardware demand\n",
    "- back up per partition is also faster \n",
    "- the other option is to split the table columns up\n",
    "\n",
    "### database  sharding\n",
    "- when we apply horizontal scaling on db level\n",
    "- there would be a table with shard key, and where they are stored, this require more setting up \n",
    "- range strategy is diving by range e.g.years, sequential shard, data might be unblanced when there is seasonality\n",
    "- hashing strategy introduce random element for distribution, it reduce hotspot bcs no cluster would be more used, but rebalancing and manging mmight be difficult, when looking up a value the results are all over places\n",
    "- functional partitioning, not putting all the tables in the same db, e.g. seperate tables that needs extra security\n",
    "- keeping data geographically close can reduce latency when using\n",
    "\n",
    "**note: file group**\n",
    "- can't be used in Azure SQL db (only primary file group there)\n",
    "- file can be putted in file group to define partition range rules and where to store\n",
    "\n",
    "### compression for table\n",
    "- pro reduced space\n",
    "- con takes extra power and time to compress and retrieve \n",
    "- can't be used on table with sparse columns\n",
    "- changing compression require droping clustered index\n",
    "- complete index viewed can be compressed\n",
    "- system table can't be compressed\n",
    "- table with different partition can be compressed differently\n",
    "- *exec sp_estimate_data_compression_savings* help estimate compression effect\n",
    "\n",
    "method:\n",
    "1. no compression\n",
    "2. row compression: char and nchar columns would be greatly reduced, varchar and int not so much, date types depends, tiny int not at all\n",
    "3. page compression which includes (below element can't be activated seperately):\n",
    "    1. row compression\n",
    "    2. prefix compression: store long duplicated value in a specific column as prefix key, and generate an extra table for prefix key to long name to prevent the duplication of long char storage\n",
    "    3. dictionary compression: similar to prefix compression, but instead of on a specific column, the duplication would be searched across columns and keys would be used for whole table\n",
    "\n",
    "    - page is a set of row up to 8192 character\n",
    "    - uncompressed at first, when additional rows can't be fit in, compression takes place\n",
    "    - some older version SQL server on VM is this unavailable\n",
    "\n",
    "**note heap**\n",
    "haep is table without clustered index\n",
    "\n",
    "**columnstore table**\n",
    "above are row stored table. in columnstore table, columns are always compress, which could be further compressed through columnstore archival compression (but this has super high compute cost for uncompress)\n",
    "\n",
    "### SQL data sync\n",
    "- works for Azure SQL, on-prem or VM, but not with management instance\n",
    "- allows sync across db\n",
    "- primary key is required\n",
    "- must define a hub db and multiple member dbs\n",
    "- rules can be defined which direction to sync, e.g. change would be replicated from hub to member or member to hub, but not directly between members\n",
    "- rule when hub and member simultaneously changed should be defined\n",
    "- sync metadata db is an azure sql db whihc loacte in the same region as hub and store, should be empty at start\n",
    "- for on perm or vm member db, a syn agent programm is needed \n",
    "- on azure db page syncing db could be created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c84b1",
   "metadata": {},
   "source": [
    "# lesson 7 strategy of migration \n",
    "\n",
    "### migration tools\n",
    "\n",
    "| purpose    | tool | extra function |\n",
    "| -------- | ------- | ------- |\n",
    "| lift & shift sql server to vm   | azure migrate    |   assess sql data estate at scale(across data center)/getting azure sql deployment recommendation, target sizing & monthly estimate  |\n",
    "| non sql objects (DB2, MYSQL, Oracle, SAP, ASE) to sql/azure sql, each are independent download program | sql server migration system     ||\n",
    "| sql server object to sql db/managed instance/vm/on prem     | data migration system    |upgrade SQL/assess sql data estate at scale/ performance & reliability improvement recommendation/ detect incompatibility of target/ migrate on-prime/ discover new feature on target|\n",
    "| compare workloads between source and target  | database experiementation assistant    | |\n",
    "| open source db (mysql, postgresql, mariadb) to azure offline/online (premium price tier)  | azure db migration service    ||\n",
    "\n",
    "1. azure migrate  \n",
    "- consists of 2 tools: discovery & assessment, server migration\n",
    "**azure migrant hub** includes azure migration assistant & azure db migration service\n",
    "\n",
    "2. sql server migration system \n",
    "\n",
    "3. data migration system\n",
    "- for large size data azure db migration service  would be better option\n",
    "\n",
    "4. database experiementation assistant \n",
    "\n",
    "5. azure db migration service \n",
    "- princing tier standard provides up to 4 vcore for free, but only support offline migration\n",
    "- princing tier premium provides up to 4 vcore with 6 month free trial, support offline & online\n",
    "\n",
    "| from    | to | online (continuous sync)    | offline (one-time) |\n",
    "| -------- | ------- | -------- | ------- \n",
    "| SQL server  | azure sql db mi    | v  | v    |\n",
    "| SQL server    | azure sql db    | x  | v    |\n",
    "| SQL server    | azure sql vm  | x  | v    |\n",
    "| mongodb    | azure cosmos db  | v | v    |\n",
    "| mySQL    | azure db for mySQL  | x  | v   |\n",
    "| postgreSQL    | azure db for postgresql  | v  | x  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**reminder** if not latest SQL is needed, then VM must be used and a window version needed to be decide\n",
    "\n",
    "**note** \n",
    "1. without downtime allowance, the only way is to do an online migration\n",
    "2. if cross db dependecy exist, azure db is not a good option, as it doesn't allow cross db queries\n",
    "3. for azure db migration service, allow outbond port 443 = port for https or 1434 used for UDP, then enable TCP IP protocal, next create azure SQL db instance which needs a server level firewall rule on depature db/server & contorl server permission on depature server to allow access to DMS and on target db control db permission is needed. it use exsiting full log backup instead of creating new one\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db218",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
