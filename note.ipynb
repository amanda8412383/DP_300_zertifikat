{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1e27f3",
   "metadata": {},
   "source": [
    "# lesson 6 configure azure sql db for scale and performance\n",
    "\n",
    "**note:backup storage redundancy** \n",
    "\n",
    "geo redundant backup will replicate the storage to a paired region (e.g. US East to Us West) in case of data storage damaging\n",
    "\n",
    "**note:billing type** \n",
    "- provisional charged based on vcore\n",
    "- serverless charged based on second, shut down after 1 hr inactivity, restarted takes a little time\n",
    "\n",
    "\n",
    "**note:hybrid benefit** \n",
    "- it's a Microsoft licensing program that allows customers to use their existing on-premises Windows Server and SQL Server licenses with active Software Assurance or subscription to reduce costs when running those workloads in Azure\n",
    "- can only work with vcore, not with dtu\n",
    "**note:elastic pool** \n",
    "\n",
    "- let manage multiple db resource allocation possible, so they could share resource when one not using it\n",
    "- not possible for hyperscale\n",
    "- can select edtu (elastic dtu) in setting\n",
    "- unit price for edut compared to dtu is extra 50 %, vcore have same unti price\n",
    "- if a DTU db peak at different time, elastic pool could be a good idea, otherwise it's more expensive due to unit price\n",
    "- unavailable at hyperscale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### storage purchasing model\n",
    "- can be changed later, except changed out of hyperscale\n",
    "- change should be done at a low trafic time\n",
    "- after change DMV(dynamic management view) needed to be flushed for accurate estimation\n",
    "exec sq_query_store_flush\n",
    "##### DTU (database transaction unit)based\n",
    "- DTU can be adjusted with related S tier\n",
    "- CDC (changed data captured) can't be used when having less than 100 DTB (= S3 = 1vcore)\n",
    "- when using more than 300 DTU, vcore might be a cheaper option\n",
    "- plan selection can based on peak usage and storage need, the latter could be estimate with average data use * db number \n",
    "1. Basic: up to 2GB\n",
    "Development, testing, infrequently accessed workload\n",
    "2. Standard\n",
    "3. Premium\n",
    "suitable when more iops (inputs output per second) is needed\n",
    "##### vCore (virtual core) based\n",
    "- max data storgae and core could be specified\n",
    "- core decide max data storgae ceiling & tempdb size\n",
    "- log space is 30% of the max data stoarge\n",
    "- hardware choice affect memory, core number and storage\n",
    "- iops = concurrent number of request, this can't be configured, but go up with vcore\n",
    "\n",
    "4. general purpose: 80 vcore, 4 terabyte storage\n",
    "suitable for low latency input\n",
    "5. Hyperscale: 80 vcore, 100 terabyte storage\n",
    "6. Business critical: 80 vcore, 4 terabyte storage, fast geo-recovery and advanced data corruption protection, free second read-only replica\n",
    "Business critical has high transaction rate and high resiliency, is suitable for large number/long-running transaction\n",
    "\n",
    "### table partitioning\n",
    "\n",
    "- a table have 3 elements storage space, computing resource limit (exceeing might lead to timeout) and network bandwidth limit\n",
    "- vertical scaling by adding disc space, processing power, memory and network connection could be a short term solution\n",
    "- horizontal scaling by divifing the table up according to rule e.g. by year, it add up complexity but reduce hardware demand\n",
    "- back up per partition is also faster \n",
    "- the other option is to split the table columns up\n",
    "\n",
    "### database  sharding\n",
    "- when we apply horizontal scaling on db level\n",
    "- there would be a table with shard key, and where they are stored, this require more setting up \n",
    "- range strategy is diving by range e.g.years, sequential shard, data might be unblanced when there is seasonality\n",
    "- hashing strategy introduce random element for distribution, it reduce hotspot bcs no cluster would be more used, but rebalancing and manging mmight be difficult, when looking up a value the results are all over places\n",
    "- functional partitioning, not putting all the tables in the same db, e.g. seperate tables that needs extra security\n",
    "- keeping data geographically close can reduce latency when using\n",
    "\n",
    "**note: file group**\n",
    "- can't be used in Azure SQL db (only primary file group there)\n",
    "- file can be putted in file group to define partition range rules and where to store\n",
    "\n",
    "### compression for table\n",
    "- pro reduced space\n",
    "- con takes extra power and time to compress and retrieve \n",
    "- can't be used on table with sparse columns\n",
    "- changing compression require droping clustered index\n",
    "- complete index viewed can be compressed\n",
    "- system table can't be compressed\n",
    "- table with different partition can be compressed differently\n",
    "- *exec sp_estimate_data_compression_savings* help estimate compression effect\n",
    "\n",
    "method:\n",
    "1. no compression\n",
    "2. row compression: char and nchar columns would be greatly reduced, varchar and int not so much, date types depends, tiny int not at all\n",
    "3. page compression which includes (below element can't be activated seperately):\n",
    "    1. row compression\n",
    "    2. prefix compression: store long duplicated value in a specific column as prefix key, and generate an extra table for prefix key to long name to prevent the duplication of long char storage\n",
    "    3. dictionary compression: similar to prefix compression, but instead of on a specific column, the duplication would be searched across columns and keys would be used for whole table\n",
    "\n",
    "    - page is a set of row up to 8192 character\n",
    "    - uncompressed at first, when additional rows can't be fit in, compression takes place\n",
    "    - some older version SQL server on VM is this unavailable\n",
    "\n",
    "**note heap**\n",
    "haep is table without clustered index\n",
    "\n",
    "**columnstore table**\n",
    "above are row stored table. in columnstore table, columns are always compress, which could be further compressed through columnstore archival compression (but this has super high compute cost for uncompress)\n",
    "\n",
    "### SQL data sync\n",
    "- works for Azure SQL, on-prem or VM, but not with management instance\n",
    "- allows sync across db\n",
    "- primary key is required\n",
    "- must define a hub db and multiple member dbs\n",
    "- rules can be defined which direction to sync, e.g. change would be replicated from hub to member or member to hub, but not directly between members\n",
    "- rule when hub and member simultaneously changed should be defined\n",
    "- sync metadata db is an azure sql db whihc loacte in the same region as hub and store, should be empty at start\n",
    "- for on perm or vm member db, a sync agent programm is needed \n",
    "- on azure db page syncing db could be created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c84b1",
   "metadata": {},
   "source": [
    "# lesson 7 strategy of migration \n",
    "\n",
    "### migration tools\n",
    "\n",
    "| purpose    | tool | extra function |\n",
    "| -------- | ------- | ------- |\n",
    "| lift & shift sql server to vm   | azure migrate    |   assess sql data estate at scale(across data center)/getting azure sql deployment recommendation, target sizing & monthly estimate  |\n",
    "| non sql objects (DB2, MYSQL, Oracle, SAP, ASE) to sql/azure sql, each are independent download program | sql server migration system     ||\n",
    "| sql server object to sql db/managed instance/vm/on prem     | data migration system    |upgrade SQL/assess sql data estate at scale/ performance & reliability improvement recommendation/ detect incompatibility of target/ migrate on-prime/ discover new feature on target|\n",
    "| compare workloads between source and target  | database experiementation assistant    | |\n",
    "| open source db (mysql, postgresql, mariadb) to azure offline/online (premium price tier)  | azure db migration service    ||\n",
    "\n",
    "1. azure migrate  \n",
    "- consists of 2 tools: discovery & assessment, server migration\n",
    "**azure migrant hub** includes azure migration assistant & azure db migration service\n",
    "\n",
    "2. sql server migration system \n",
    "\n",
    "3. data migration system\n",
    "- for large size data azure db migration service  would be better option\n",
    "\n",
    "4. database experiementation assistant \n",
    "\n",
    "5. azure db migration service \n",
    "- princing tier standard provides up to 4 vcore for free, but only support offline migration\n",
    "- princing tier premium provides up to 4 vcore with 6 month free trial, support offline & online\n",
    "\n",
    "| from    | to | online (continuous sync)    | offline (one-time) |\n",
    "| -------- | ------- | -------- | ------- \n",
    "| SQL server  | azure sql db    | x  | v    |\n",
    "| SQL server    | azure sql db mi    | v | v    |\n",
    "| SQL server    | azure sql vm  | x  | v    |\n",
    "| mongodb    | azure cosmos db  | v | v    |\n",
    "| mySQL    | azure db for mySQL  | x  | v   |\n",
    "| postgreSQL    | azure db for postgresql  | v  | x  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**reminder** if not latest SQL is needed, then VM must be used and a window version needed to be decide\n",
    "\n",
    "**note** \n",
    "- without downtime allowance, the only way is to do an online migration\n",
    "- if cross db dependecy exist, azure db is not a good option, as it doesn't allow cross db queries\n",
    "- for azure db migration service, allow outbond port 443 = port for https or 1434 used for UDP, then enable TCP IP protocal, next create azure SQL db instance which needs a server level firewall rule on depature db/server & contorl server permission on depature server to allow access to DMS and on target db control db permission is needed. it use exsiting full log backup instead of creating new one\n",
    "- there are other service e.g. Bulk copie programm, command bulk insert, log data from Azure blob storage, SQL server integration service package, spark/azure factory\n",
    "- migrant stuff directly between Azure SQL db can use export or export data tier application (which allow export of schema), or export form azure portal, download and using SQLpackage\n",
    "- export data tier application (DAC) will make the export as .bacpac (backup package) file\n",
    "\n",
    "### post migration\n",
    "1. have tests to make sure data are there\n",
    "2. checking Query returns same result in sourece and target db\n",
    "3. checking the performance\n",
    "4. changing the app connecting to db from departure db to target db\n",
    "5. checking missin features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db218",
   "metadata": {},
   "source": [
    "# lesson 8 Microsoft Entra ID\n",
    "\n",
    "\n",
    "### authn = authentication = log in\n",
    "1. window authentication\n",
    "    - on a window machine/domain \n",
    "    - in azure only on VM is window available\n",
    "2. SQL sever authentication\n",
    "    - using username and password\n",
    "3. azure active directory\n",
    "    - setting could be access through azure portal, e.g. adding user\n",
    "    - more secure than SQL sever authentication and can select way to authenticate in portal \n",
    "    - once the AAD admin is setted through azure portal (only 1 admin allow), the admin can log in SSMS through (other log in method won't allow role creation) and create more roles through query \n",
    "    - comparison between AAD admin and sql server admin:\n",
    "        - both could create users based on SQL server authentication log in\n",
    "        - both can create contained DB users based on SQL server authentication wo log in\n",
    "        - AAD admin can create contained DB users based on aad users and group\n",
    "    - authentication methods includes FIDO secutity keys which is a hardware on computer, microsoft authenticator, text message and temporary access pass\n",
    "    - can sync with on-prem windows server active directory\n",
    "    - when we are logging in azure portal is a form of azure active directory\n",
    "    - azure active directory - integrated/universal with MFA(multi factor authentication) can be used when using admin tool like SSMS on a computer that is not domain joined \n",
    "    - 3 different way to authenticate in:\n",
    "        1. cloud only identity\n",
    "            - handle sign in completely in cloud by azure\n",
    "        2. federated authentication\n",
    "            - integrate with existing federation provider\n",
    "            - sign in requirement which is not native supported by Azure active directory\n",
    "        3. pass-through authentication\n",
    "            - when don't have a sign-in requirement not natively supported by Azure active directory\n",
    "            - and want to enforce extra feature like user level active directory sign in policy\n",
    "    - identity: \n",
    "        1. cloud only identity\n",
    "        2. hybrid identity support both cloud authentication & pass-through authentication\n",
    "        3. hybrid identity support federated authentication\n",
    "    - to create a user based on AAD user use: create user [user_email] from external provider\n",
    "4. window server active directory\n",
    "\n",
    "**note:**\n",
    "- authz = autherization = what one could access\n",
    "\n",
    "### role\n",
    "-  when running on an non-azure machine that is domain joined by creating a certificate and connect the app to azure data\n",
    "- to see all role in azure SQL use sp_helprole\n",
    "- after creating user, adding roles for the user is needed in SQL server\n",
    "    - alter role [role name] add/drop member [member email]\n",
    "    1. db_owner : have access to all.\n",
    "    2. db_securityadmin : modify role membership for custom roles only. manage permission, can elevate own permission\n",
    "    3. db_accessadmin: add & remove access to db for login & groups\n",
    "    4. db_backup_operator: not applicable in azure. can backup db, mi & vm\n",
    "    5. db_ddladmin: can run ddl comment (creat, alter, drop)\n",
    "    6. db_data_reader: can read all data\n",
    "    7. db_deny_data_reader: can't read old data\n",
    "    8. db_data_writer: ability to add, delete or change data\n",
    "    9. db_deny_data_writer: can't add, delete or change data\n",
    "    - on-prem only, roles that exist in master db\n",
    "    10. db_manager: create/delete db\n",
    "    11. login_manage: create/delete login in master db = security_admin for master db\n",
    "- role based access control (RBAC) in azure:\n",
    "    - Access control section is called IAM, when role could be added\n",
    "    - there is no access to server wide logging permission bc access to underlying server doesn't exist\n",
    "    1. SQL DB contributer: allow SQL DB, but don't have access to content\n",
    "    2. SQL server contributer: allow SQL server/security policy manage, but don't have access to content\n",
    "    3. SQL security manager: allow SQL security policy manage, but don't have access to content\n",
    "\n",
    "### granting permission\n",
    "- granting access to particular schema/table can't be achieved through these set roles in stead grant permission command should be used\n",
    "- command: authorization permission on securable::name to pricipal with grant option\n",
    "- select * from sys.fn_my_permissions(null, 'DATABASE') shows all permission\n",
    "1. autherization (grant, revoke, deny)\n",
    "    - grant [permission_type] on object::[schema].[table] to [user_email]\n",
    "    - to revoke the permission used revoke [permission_type] on object::[schema].[table] to [user_email]\n",
    "    - to ban someone from table access that cannot be overwrite by grant use deny [permission_type] on object::[schema].[table] to [user_email]\n",
    "2. permission \n",
    "    - table: select, insert, update, delete, control(all right to table), reference(view foreignkey), take ownership(change ownership), view change tracking, view definition (rightclick and go to script table path)\n",
    "    - schema: alter(=alter/create/drop any securable)\n",
    "    - function/stored procedure: alter, execute(even lack of select permission on table is not a problem, this is called ownership changing), view change tracking, view definition\n",
    "3. securable (object, schema, db, server)\n",
    "4. principal (login, user, role: create role [rolename], add member [useremail])\n",
    "    - public is a special role that every single user of the db have this role\n",
    "5. with grant option: allow the principal to share the permission later\n",
    "\n",
    "**note:apply principal of least privilige for all securable**\n",
    "a user should have least privilige user account (LUA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1cbe2",
   "metadata": {},
   "source": [
    "# lesson 9 implement security\n",
    "\n",
    "### transparent data encryption (tde)\n",
    "- this is the security for data in transit\n",
    "- it encrypts then deencrypts data at page level at rest\n",
    "- encrypts at write then decrypts at read\n",
    "- for azure SQL db this is set at server level, new db inheritate the rule unless recover from a copy, encryption can be turn off in azure portal / Tsql / cloudshell / restapi\n",
    "- for azure sql mi this is set at instance level,  new db inheritate the rule but not system db, encryption can be turn off  in Tsql/ cloudshell/ restapi\n",
    "- it use database encryption key (dek), which is a symetric key = one single key is needed for en- & decrypts\n",
    "    - this could be a service managed key, protected by a TDE protector using service managed certificate \n",
    "    - this could be a bring your own key, in this case it could also be asymmetric\n",
    "\n",
    "**note** transparent layer security (TLS), encrypts when in transit\n",
    "\n",
    "### object level encryption (e.g. table with sensitive data)\n",
    "1. always encrypted\n",
    "    - always encrypted is available in azure sql db, mi, vm\n",
    "    - this could be done in ssms through right click encrypt columns\n",
    "    - encryption type has deterministic (encryption is the same everytime, allows for join, group by, index, distinct without decrypt) & random \n",
    "    - when the column is used as index, it can't be randomized encrypt\n",
    "    - to create encryption, key vault is needed\n",
    "    - after creation, next time when connecting to db in SSMS select enable alway encrypt \n",
    "    - a pricipal (e.g. user) need to be in key vault to decrypt encrypted colunm\n",
    "\n",
    "    - to query an encrypted column in where clause, set up a parameter & enable parameterlization for always encrypted is needed, but this will still block funtion like\n",
    "    - to enable partial string comparison, always encrypted need to be used with secure enclaves\n",
    "        1. enable confidential computing through configured as DC series\n",
    "            - strongest security isolation\n",
    "            - type name is intel software guard intellegence(sgx)\n",
    "            - require ms azure attestation\n",
    "        2. enable secure enclaves in a standard configure db\n",
    "            - type name is virtualization based security (vbs)\n",
    "            - available for all ay sql, and sql 2019+\n",
    "            - not available in ger india central\n",
    "            - can't defend against e.g. replacing enclaves program with malware\n",
    "            - creating a protected part of memory within which data is display as plain text\n",
    "            - protection against operative system threat\n",
    "            - standard azure protection (e.g. multifactor authentication, just in time access)\n",
    "            - can't be deactivated after activation\n",
    "\n",
    "    - permission wise there are\n",
    "        1. alter any column master key\n",
    "        2. alter any column encryption key\n",
    "        3. view any column master/encryption key (for access, query and read encrypted data)\n",
    "    - role wise there are\n",
    "        1. security admin who generate column encryption key and master key, access to key and key store are required, access to db is not required\n",
    "        2. db admin who manage metadata about key, access to key and key store isn't required, access to db is required\n",
    "        - if these 2 role should belongs to 2 ppl, then powershell should be used\n",
    "        - if these 2 role should belongs to same ppl, then powershell or ssms could be used\n",
    "\n",
    "2. dynamic data masking\n",
    "    - this could be done through azure portal \n",
    "    - in tsql: alter table [schema].[table] alter column [column] add masked with (fuction = 'default()')\n",
    "    - it can be apply on specific column with desired partial masking 2025-0x-1x\n",
    "    - specific users could be excluded in azure portal/tsql: grant(/revoke) unmask to [useremail] \n",
    "    - this could also be done through powershell\n",
    "    - mask type\n",
    "        - default value 0\n",
    "        - number: choose random number between 2 different bundary\n",
    "        - email: expose first value of email\n",
    "        - credit card: last 4 digits of card number\n",
    "### key vault \n",
    "- could be created in azure sql portal\n",
    "- in key vault store secrets\n",
    "- key vault name need to be unique\n",
    "- better store it in region near db\n",
    "- pricing tier premium allow use of hardware security model (HSM). it's the same as standard  until hsm is used\n",
    "- in access policy selecting unwrap key + wrap key + verify + sign (and also the default selections) is necessary to column create primary key \n",
    "\n",
    "\n",
    "| | always encrpted    | transparent data encrpytion (tde) |\n",
    "| -------- | ------- |------- |\n",
    "| sql server version  |  2016+   | 2008+ |\n",
    "| sql server entriprise edition | x     | v|\n",
    "| free in azure sql db    | v    | v|\n",
    "| protect data at rest   | v   | v |\n",
    "| protect data in use    | v  | x (when in transport, using transport layer security) |\n",
    "| protect data from sql admin  | v  | x |\n",
    "| data is de/encrypted on the client side  | v | x|\n",
    "| data is de/encrypted on the server side   | x | v |\n",
    "| encrypt at    | column level  | entire db|\n",
    "| transparent to application   | partially | v |\n",
    "| encryption option   | v | x |\n",
    "| key management   | customer managed keys | server/customer managed keys |\n",
    "| protects key in use  | v | x |\n",
    "| driver required | v |  x|\n",
    "\n",
    "\n",
    "### configure server/db level firewall\n",
    "- by default all connection are rejected, unless firewall is set up\n",
    "- db rule would be checked before server rule when implement\n",
    "- server rule need to be set before db rule\n",
    "- doesn't apply to azure sql mi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|| server   | db |\n",
    "| ------- |------- |------- |\n",
    "|| for user and app to have access to all dbs in the server| for an individual/app |\n",
    "|can be set in azure portal|v | x|\n",
    "|role|sql server contributor/sql security manager/owner of the resource | control db permission is required at db level|\n",
    "|tsql to check rules|select * from sys.firewall_rules  | select * from sys.database_firewall_rules |\n",
    "|tsql to set rules|execute sp_set_firewall_rule @name=N'rule', @start_ip_address='ip', @end_ip_address='ip' (need to be in masterdb)| execute sp_set_database_firewall_rule @name=N'rule', @start_ip_address='ip', @end_ip_address='ip' |\n",
    "|tsql to delete rules|execute sp_delete_firewall_rule @name=N'rule' (need to be in masterdb)| execute sp_delete_database_firewall_rule @name=N'rule'|\n",
    "\n",
    "**note** n before quote change varchar to nchar/nvarchar\n",
    "\n",
    "### transport layer security (tls)\n",
    "- encrypt data on db/client side and decrypt it from other side\n",
    "- most widely used is TLS 1,2, available in 2008 and is used in azure sql db\n",
    "- if the tls version from cleint older than min version allowed in target db , connection would fail\n",
    "- min tls version can be set in powershell/azureportal\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2245f",
   "metadata": {},
   "source": [
    "# lesson 10 & 11 implement compliance control for sensitive data\n",
    "\n",
    "### data classification\n",
    "- sensitive data include data privacy, regulatory and national requirement\n",
    "- classification can be set in azure portal, it will also show recommanded classification \n",
    "- information type and security label can't both be n/a\n",
    "- security label:\n",
    "    - n/a : available to all \n",
    "    - public: public released data\n",
    "    - general: data not mean for public e.g. emails, docs\n",
    "    - confidential\n",
    "    - confidential - GDPR\n",
    "    - highly confidential\n",
    "    - highly confidential - GDPR\n",
    "- role to change classification: db_owner, db_contributor, sql_security_manager\n",
    "- role to read classification: reader, user access admin\n",
    "- tsql to check: select * from sys.sensitivity_classifications\n",
    "- tsql: select * from sys.columns where object_id = [majorid from above query]\n",
    "- tsql to add: add sensitivity classification to [schema].[table].[column name] (with label='label name', information_type='info name', rank=low/medium/high/critical/none)\n",
    "- tsql to drop: drop sensitivity classification to [schema].[table].[column name] \n",
    "\n",
    "### configure server and db audits\n",
    "- audit retain trails of selected db actions, report db activity using preconfigured report on dashboard and analyse report for suspicious event\n",
    "- not supported for premium storage/hierarchical namespace\n",
    "- having server and db audit at the same time is possible, but only using server level is recommand, unless specific requirement for specific db is needed\n",
    "- default policy includes batch completed group, all the queries & storage procedure, successful db & failed db authentication group (e.g. login)\n",
    "- it store around 4000 character in an audit \n",
    "- server policy audit always apply to db disregarding db level audit policy\n",
    "- audit log storage:\n",
    "    1. existing/ new storage account\n",
    "    2. existing monitor log analytics workspace\n",
    "    3. existing event hub\n",
    "- audit log can be viewed in azure portal or ssms merge audit file and add from azure blob storage\n",
    "\n",
    "### data change tracking\n",
    "- both can be used in azure sql db, but only change data capture can be usedd in aure sql mi\n",
    "1. change tracking (ct)\n",
    "    - when a row/column get changed it will be tracked\n",
    "    - doesn't track how many time it has changed/history data\n",
    "    - it store less data than adc\n",
    "    - it store data in an in-memory roaster and flushed on every checkpoint to the internal data (it kept in memory and then saved often)\n",
    "    - snapshot isolation\n",
    "        - SQL Server recommends using snapshot isolation with change tracking\n",
    "        - Snapshot isolation ensures that the change tracking process itself operates on a consistent snapshot of the data, preventing issues where changes might be missed or incorrectly reported due to concurrent modifications \n",
    "        - using snapshot isolation will makes any changes while geting data invisible (it has a set of data instead of a changing set)\n",
    "        - tsql for snapshot isolation:ALTER DATABASE [dbname] SET ALLOW_SNAPSHOT_ISOLATION ON then SET TRANSACTION ISOLATION LEVEL SNAPSHOT\n",
    "    - enable ct can be done in ssms /tsql, it has to be done first on db level then for specific tables\n",
    "    - for tracking updates command, extra parameter need to be set\n",
    "    - tsql for enabling ct: alter database mydb set change_tracking = on (change_retention = 2 days, auto_cleanup = on)\n",
    "    - to disable, all tables need to be disabled then the db\n",
    "    - tsql for Check which tables/databases have CT enabled: \n",
    "    ```\n",
    "        SELECT * from sys.change_tracking_databases \n",
    "        select * from sys.databases \n",
    "            -- for azure sql db only  master & db available, no temp db, model & ms db\n",
    "        SELECT * from sys.change_tracking_tables  \n",
    "        select * from sys.objects\n",
    "            -- all table, queries and other objects \n",
    "    ```\n",
    "    - tsql for getting the initial sync version:\n",
    "\n",
    "        DECLARE @last_sync bigint; \n",
    "        SET @last_sync = CHANGE_TRACKING_CURRENT_VERSION();  \n",
    "        select @last_sync \n",
    "        select CHANGE_TRACKING_CURRENT_VERSION() \n",
    "\n",
    "    - tsql for finding out what after changes have happened: \n",
    "\n",
    "        SELECT  CT.AddressID, CT.SYS_CHANGE_OPERATION,   \n",
    "        CT.SYS_CHANGE_COLUMNS, CT.SYS_CHANGE_CONTEXT \n",
    "        FROM  CHANGETABLE(CHANGES SalesLT.Address, 0) AS CT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. change data capture(cdc)\n",
    "- not available in basic price tier, available in standard price tier from s3\n",
    "- need to first enable for db then for table\n",
    "- tsql for Enable for database (can't be used in system db) :\n",
    "    EXEC sys.sp_cdc_enable_db  \n",
    "- tsql for Enable for table :\n",
    "\n",
    "    EXEC sys.sp_cdc_enable_table @source_schema = 'SalesLT', @source_name = 'Address', \n",
    "    @role_name = 'NewRole', \n",
    "    @captured_column_list = 'AddressID, City'\n",
    "\n",
    "- tsql for checking configuration:\n",
    "\n",
    "    EXECUTE sys.sp_cdc_help_change_data_capture \n",
    "\n",
    "- tsql for Disable from table :\n",
    "\n",
    "    EXEC sys.sp_cdc_disable_table @source_schema = 'SalesLT', @source_name = 'Address', \n",
    "    @capture_instance = 'all' \n",
    "\n",
    "- tsql for Disable for database :\n",
    "\n",
    "    EXEC sys.sp_cdc_disable_db \n",
    "\n",
    "- tsql for checking changes:\n",
    "\n",
    "    DECLARE @from_lsn binary(10), @to_lsn binary(10);   \n",
    "    SET @from_lsn = sys.fn_cdc_get_min_lsn('SalesLT_Address');   \n",
    "    SET @to_lsn = sys.fn_cdc_get_max_lsn();   \n",
    "    SELECT * FROM cdc.fn_cdc_get_all_changes_SalesLT_Address  (@from_lsn, @to_lsn, N'all')\n",
    "\n",
    "    **note** lsn = log sql number\n",
    "\n",
    "    **note** cdc.fn_cdc_get_all_changes_SalesLT_Address is a custome sql function made by cdc, it has name of the specific table, argument are from, to , and display type\n",
    "\n",
    "### vulnerability assessment\n",
    "- a defender for Servers plan is required to do assessment, azure defender cost 15 dollar per month per server\n",
    "- adding a failed test as baseline will change the failing status\n",
    "\n",
    "### azure purview\n",
    "- when data is stored in different places it's hard to check for compliance\n",
    "- purview catalogues the data regardless of on premises, in a machine, online, on cloud using ssas\n",
    "- accessing new data source from purview\n",
    "    1. azure service need to allow connection from azure service in server setting for purview to work\n",
    "    2. a secret (e.g. sql password) in keyvault (using purview resource group) need to be added\n",
    "    3. in access policy from key vault give permission to purview, with key permission get & list\n",
    "    4. in purview credential, add the secret from step 2\n",
    "- using purview\n",
    "    1. create ms purview account (need subscription & resource group )\n",
    "    2. open ms purview portal which shows 3 main element:\n",
    "        1. data map: matadata created through classifying and scanning varios sources\n",
    "        2. data catalogue: help with finding data with classification/metadata filter\n",
    "        3. data insight: locate sensitive data\n",
    "    3. using scan rule set which help group together classification & file type\n",
    "- collection & scan \n",
    "    - collection within collection is allowed\n",
    "    1. add a source & selecting which source it should be linked to\n",
    "    2. click scan on the added source, define scan table, targeted data type & time schedule\n",
    "    - lineage extraction is limited to store procedure run, otherwise should be turned off\n",
    "\n",
    "\n",
    "### azure db ledger\n",
    "\n",
    "- ledger provides a history of changes made to a database's data, ensuring data integrity and enabling auditing\n",
    "\n",
    "**what is updatable ledger table**\n",
    "- updatable ledger table provide cryptogrphic (a security communication technique) proof of date to the auditors. it reduce time to audit data. \n",
    "- all the modification that is done to updatable ledger table is hashed cryptographically using SHA256 (= a root hash). \n",
    "- a hash is similar to a footprint that is left behind by the footstep. root hash are stored in blocks. blocks are closed after 30 sec/ 100,000 transactions, then hashed along with a root hash pf ürevious block, forming a blockchain0.\n",
    "- the latest hash block is called database digest\n",
    "**how updatable ledger table is stored**\n",
    "- db ledgers are stored in trusted storage e.g. immutable azure blob storage(which follows write one read many times = can't be altered), azure credential ledger\n",
    "- data integrity can be verified by comparing db digest hash against calculation of current table\n",
    "- sql db manage db ledger transparently\n",
    "**how updatable ledger table created**\n",
    "- non ledger table can't be converted into ledger table\n",
    "- data need too be copied into new ledger table(select into/bulk insert/sp_copy_data_in_batch, sp=stored procedure), then rename ledger table as original table name\n",
    "    - benefit of sp_copy_data_in_batch is it split copy operation into batches, 10 - 100 k per transaction done in parallel, therefore speed up the process\n",
    "1. in ssms go to table, right click, script table as\n",
    "2. copy the script to create a new table, with new name and adding tsql\n",
    "    for updatable: with (system_versioning = on, ledger = on)\n",
    "    for append only: with (ledger = on,append only = on)\n",
    "3. insert everything from original table into this ledger table\n",
    "4. now the history table can be seen in sys.table if updatable is used\n",
    "5. could query from the ledger view\n",
    "\n",
    "**what's in updatable ledger table**\n",
    "- ledger table have 4 new cols, named as always generated columns, automaticallly generated \n",
    "    1. ledger_start_transaction_id: unique transaction id of each inserted action, e.g. if 10 rows are inserted at the same time, they'll all have the same id\n",
    "    2. ledger_end_transaction_id\n",
    "    3. ledger_start_sequence_id: e.g. if 10 rows are inserted at the same time, they'll have sequence from 0 to 9. for next transaction sequence starts from 0 again\n",
    "    4. ledger_end_sequence_id\n",
    "\n",
    "**what is history table**\n",
    "- it's also built on db ledgers\n",
    "- it shows previous version of a row, when it's updated/deleted in the updatable ledger table\n",
    "- it has also 4 always generated columns\n",
    "- data can't be deleted from here\n",
    "- automatically named as original table name.mssql_ledgerhistoryfor_guid\n",
    "\n",
    "**what is ledger  view**\n",
    "- it join updatable ledger table and history table\n",
    "- it shows transaction id, delete/insert/update(update is both delete & insert)\n",
    "- is recommanded to used instead of the other 2 tables\n",
    "\n",
    "**append-only ledger table**\n",
    "- another version of ledger table, - it's also built on db ledgers\n",
    ", alternative to the updatable ledger table\n",
    "- data can be insert but not update/delete\n",
    "- in this case history table is not needed\n",
    "- there is still a ledger view, provide info on transaction & user who inserted data\n",
    "- 2 always generated columns\n",
    "    1. ledger_start_transaction_id: \n",
    "    2. ledger_start_sequence_id: \n",
    "\n",
    "**verifying**\n",
    "1. in azure portal, select digest storage, adding a storage container\n",
    "2. go to verify db option, which provides code to look at data_ledger_digest_location \n",
    "    - storage container could also be activated in security tab when creating azure sql db\n",
    "    - when creating azure sql db if select ledger db, then all the table in this db will by default all and only be ledger tables\n",
    "3. copy this and run in ssms, it'll show:\n",
    "    - digest location (path of the digest)\n",
    "    - last verified blog id \n",
    "    - whether the path is the latest location, if unsuccessful then db has been tempered with\n",
    "4. if it's been tampered with restored to a verifiable version is needed, then manually create any future transaction while using backups after the point of time\n",
    "\n",
    "\n",
    "### row level security\n",
    "- available for sql 2016 +\n",
    "- actions like select, update and delete could be restricted to specific rows\n",
    "- block write operations on new/existing data: after insert/after update\n",
    "- block update/delete on existing data: before delete/before update\n",
    "1.  it's recommanded to have a seperate schema for row level security object, it helps reducing effort in maintaining various permission \n",
    "2. create inline table valued function (tvf), it returns when user should see the result: \n",
    "\n",
    "    create function rls.rls_security(@user an nvarchar(10)) returns table with schema binding \n",
    "\n",
    "    as return select 1 as result \n",
    "\n",
    "    where @user = username()\n",
    "3. add select permission to the tvf: \n",
    "\n",
    "    grant select on rls.rls_security to [user1] \n",
    "\n",
    "    grant select on sceham.table to [user1]\n",
    "4. create security policy (role needed): \n",
    "\n",
    "    create security policy policyname \n",
    "    \n",
    "    add filter predicate rls.rls_security(usercolname )\n",
    "    on sceham.table\n",
    "\n",
    "    **note** this filter out what shouldn't be seen\n",
    "\n",
    "    add block predicate rls.rls_security(usercolname )\n",
    "    on sceham.table after insert\n",
    "\n",
    "    **note**  block insert outside of what allowed to be seen\n",
    "\n",
    "    with (state = on)\n",
    "\n",
    "    go\n",
    "- without turining the security policy on, everyone will be able to view all rows, after turning on, only the user being granted access can\n",
    "\n",
    "### configure advanced threat protection\n",
    "- available for azure sql db, mi, vm\n",
    "- it allow admin to be alerted when potential threat happened e.g. suspicious pattern on querying/access, possible vunerability, sql injection attack\n",
    "1. enable ms defender for cloud\n",
    "2. click on the configure and set up email notification for advanced threat protection alert\n",
    "3. the email will also show the suggested action for fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49534a6",
   "metadata": {},
   "source": [
    "# lesson 12 monitoring activity and performance & lesson 17\n",
    "\n",
    "### creating performance base line\n",
    "- performance are affected by hardware (pricing tier)\n",
    "- on vm performance is also affected by operation system, db applications and client application\n",
    "- with a high dtu percentage(= processor percentage, Database Transaction Unit, it represents a blended measure of CPU, memory, reads, and writes) or i/o percentage :\n",
    "    1. more cpu/input output resource is needed \n",
    "    2. query needs to be optimized\n",
    "1. metrics explorer\n",
    "\n",
    "    - metrics are collected at regular interval and stored in a time series db\n",
    "    - they have a timestamp for when, name, value and other label\n",
    "    - it's light weighted and allows for near real time allerting\n",
    "    - in monitoring section, metric tab allows one to see metric for a specific sql db\n",
    "    - max 30 days at once \n",
    "    - up to 93 days in the past is viewable\n",
    "\n",
    "2. log\n",
    "    - operation performance metrics can also be created by using log\n",
    "    - logs can be structured or free form, may have a timestamp\n",
    "    - in monitoring> logs> diagnostic> loading data, last hour logs can be imported using language kusto\n",
    "\n",
    "3. tsql\n",
    "    - SELECT * from sys.dm_db_resource_stats  \n",
    "        - current db or elastic pool CPU, IO and memory \n",
    "        - a row every 15 s, for last 1 hr\n",
    "    - SELECT * from sys.server_resource_stats  \n",
    "        - equivalents for reports on azure sql mi, CPU, IO and memory \n",
    "        - a row every 15 s, for last 1 hr\n",
    "\n",
    "    - SELECT * from sys.resource_stats \n",
    "        - reports on server( multiple db), CPU, IO and memory \n",
    "        - a row every 15 s, for last 1 hr\n",
    "\n",
    "    - SELECT * from sys.dm_user_db_resource_governance  \n",
    "        - Returns one row actual configuration and capacity settings used by resource governance mechanisms in the current database or elastic pool\n",
    "        \n",
    "    - SELECT * from sys.resource_usage  \n",
    "        - provides hourly summary of result usage data for user db in current server\n",
    "        - even if db is idle, as long as it's accessible, it'll have 1 row/hour \n",
    "        - the view doesn't contain usage anymore, only storage in megabyte\n",
    "\n",
    "    - SELECT * FROM sys.dm_os_job_object \n",
    "        - CPU, memory and I/O resource at the SQL Server level. \n",
    "        - a job object is a windows construct that implement CPU, memory and I/O resource at operating system level\n",
    "\n",
    "\n",
    "    - SELECT * FROM sys.dm_io_virtual_file_stats(null, null) \n",
    "        - I/O statistics for data and log files \n",
    "        - could specify dbid and fileid in bracket\n",
    "\n",
    "    - SELECT * FROM sys.dm_os_performance_counters \n",
    "        - returns row of each Performance counter information on a server\n",
    "        - multiple object and counter that are also available for db replica, db mirroring \n",
    "\n",
    "\n",
    "    - SELECT * FROM sys.dm_os_wait_stats\n",
    "        - Waiting on resources \n",
    "        - log_rate_governor is wait time for azure sql db\n",
    "        - pool_log_rate_governor is wait time for elastic pool\n",
    "        - instance_log_rate_governor is wait time for mi\n",
    "        - rbio_ prefix group stat are for hyper scale log governance\n",
    "        - hadr throttle log rate for business critical and geo replication latency\n",
    "        - pageiolatch show wait time of io\n",
    "        - pagelatch show wait time of temp db io\n",
    "        - writelog transactional io issue\n",
    "        - resource_semaphore is waiting until memory is available\n",
    "        - wait issue caused by parallelism\n",
    "            - cxpacket is when max degree of parallelism is too high/ index needed\n",
    "            - sos_scheduled_yield is high cpu utilization might be caused by missing index\n",
    "\n",
    "\n",
    "    - SELECT * FROM sys.dm_db_wait_stats \n",
    "        - equivalent for Waiting on resources in azure sql db and mi, more limited\n",
    "\n",
    "\n",
    "\n",
    "    - SELECT * FROM sys.dm_exec_requests WHERE blocking_session_id <> 0 \n",
    "        - Possible blocking, session based\n",
    "    - SELECT * FROM sys.dm_os_waiting_tasks  WHERE blocking_session_id <> 0 \n",
    "        - Possible blocking, task based\n",
    "\n",
    "\n",
    "    - SELECT * FROM sys.dm_os_schedulers where STATUS = 'VISIBLE ONLINE'; \n",
    "        - show vcores\n",
    "\n",
    "    - SELECT SERVERPROPERTY('EngineEdition'); \n",
    "        - 5 is sql db, mi is 8, on-perm/vm is smaller than 5\n",
    "    \n",
    "    - exec sp_who \n",
    "        - show current user , process, whether it's blocked\n",
    "    - exec sp_lock \n",
    "        - show lock, object id, index id\n",
    "        - Locks are held on SQL Server resources, such as rows read or modified during a transaction, to prevent concurrent use of resources by different transactions   \n",
    "    - exec sp_spaceuser\n",
    "        - display estimation of current disc space use     by a table or db \n",
    "    - exec sp_monitor\n",
    "        - work for on prem db but not for azure sql db\n",
    "        - show cpu usage, io usage, idle time etc\n",
    "\n",
    "**note** An on-premises database, refers to a database system that is hosted within a company's own facilities, typically in their own data center or server room\n",
    "\n",
    "### data base console command (dbcc)\n",
    "\n",
    "- DBCC CHECKDB \n",
    "- DBCC CHECKDB(DP300) \n",
    "    - Check logical and physical integrity of all objects \n",
    "- below are the content of encompass by DBCC CHECKDB \n",
    "    - DBCC CHECKALLOC \n",
    "        - Checks the consistency of disk space allocation structures \n",
    " \n",
    "    - DBCC CHECKTABLE('[SalesLT].[Address]') \n",
    "        - Checks all pages and structure in a tables and index views \n",
    "        - e.g. whether data page are correctly linked, indexes are in correct sort order, every row in a table has a matching row in a non-clustering index and in correct partition\n",
    "    - DBCC CHECKCATALOG \n",
    "        - check catalog consistency using an internal db snapshot\n",
    "        - perform consistency check between system metadata tables\n",
    "        - doesn't run against tempdb/foul stream data (e.g. blob)\n",
    "    - check db also validate every index view in the db, link level consistency between the table metadata & file system directory and files\n",
    "- options for check db:\n",
    "    - DBCC CHECKDB (0, NOINDEX) \n",
    "        - 0 means the current db would be used\n",
    "        - noindex detects error only, which significant decrease runtime\n",
    "    - ```\n",
    "        ALTER DATABASE [DP300] \n",
    "        SET SINGLE_USER WITH ROLLBACK IMMEDIATE \n",
    "        GO \n",
    "        DBCC CHECKDB (0, REPAIR_REBUILD) \n",
    "        GO \n",
    "        SET MULTI_USER \n",
    "        GO \n",
    "        ```\n",
    "        - only does repair which has no chance of data loss\n",
    "        - need to be in single user mode to use it, and return to multi-user mode afterward\n",
    "    - ```\n",
    "        ALTER DATABASE [DP300] \n",
    "        SET EMERGENCY \n",
    "        GO \n",
    "        ALTER DATABASE [DP300] \n",
    "        SET SINGLE_USER WITH ROLLBACK IMMEDIATE \n",
    "        GO \n",
    "        DBCC CHECKDB (0, REPAIR_ALLOW_DATA_LOSS) \n",
    "        SET MULTI_USER \n",
    "        GO \n",
    "        ```\n",
    "        - repair that might cause data lose, ms recommend backup beforehand\n",
    "        in single user mode require\n",
    "        - ms also recommend set emergency which make db  read only, locking is disabled and access limited to cis admin\n",
    "    - DBCC CHECKCATALOG WITH NO_INFOMSGS \n",
    "        - supress all message\n",
    "    - ````\n",
    "        DBCC CHECKDB (0, REPAIR_ALLOW_DATA_LOSS) \n",
    "        WITH \n",
    "        ALL_ERRORMSGS  \n",
    "        , EXTENDED_LOGICAL_CHECKS -- not ALLOC \n",
    "        , NO_INFOMSGS  \n",
    "        , TABLOCK \n",
    "        , ESTIMATEONLY \n",
    "        , PHYSICAL_ONLY -- not ALLOC \n",
    "        , MAXDOP = 4 \n",
    "        ````\n",
    "        - the same goes for CHECKALLOC and CHECKTABLE \n",
    "        - ALL_ERRORMSGS with all error message display\n",
    "        - EXTENDED_LOGICAL_CHECKS perform logical consistency check on index view, xml index, spatial index\n",
    "        - TABLOCK speed things up but obtains exclusive locks(reduce concurrency)\n",
    "        - ESTIMATEONLY no db checks are done, display tempdb space needed to do it\n",
    "        - PHYSICAL_ONLY limit checking to page structure integrity, record header integrity, consistency of db\n",
    "        - MAXDOP the # of process, override max degree of parallelism \n",
    "    - best practice recommended by ms, do transaction before so you can decide to commit or rollback at the end\n",
    "    - ms also recommend fix error through backup instead of dbcc\n",
    "    - after dbcc checkdb, inspect of referential integrity of the db, check constraint in db and tables \n",
    "        - DBCC CHECKCONSTRAINTS \n",
    "\n",
    "### database configuration options \n",
    "- tsql or ssms interface both possible\n",
    "````\n",
    "ALTER DATABASE [DP300] \n",
    "SET AUTO_CLOSE ON \n",
    "ALTER DATABASE [DP300] \n",
    "SET AUTO_CREATE_STATISTICS ON \n",
    "ALTER DATABASE [DP300] \n",
    "SET AUTO_UPDATE_STATISTICS ON -- [_ASYNC] \n",
    "ALTER DATABASE [DP300] \n",
    "SET AUTO_SHRINK OFF --ON \n",
    "ALTER DATABASE [DP300] \n",
    "SET READ_WRITE --/ READ_ONLY \n",
    "ALTER DATABASE [DP300] \n",
    "SET MULTI_USER --/ RESTRICTED_USER / SINGLE_USER \n",
    "ALTER DATABASE [DP300] \n",
    "SET RECOVERY FULL --/ RECOVERY BULK_LOGGED / RECOVERY SIMPLE \n",
    "ALTER DATABASE [DP300] \n",
    "SET COMPATIBILITY_LEVEL = 150 --(SQL Server 2008 and R2), 110, 120, 130, 140, 150 (SQL \n",
    "Server 2019) \n",
    "````\n",
    "    - AUTO_CLOSE not use in azure sql db, not recommended, shut db down after last user exit\n",
    "    - AUTO_CREATE_STATISTICS create statistic on single column, inquiry, where clause, help improve query performance\n",
    "    - AUTO_UPDATE_STATISTICS query optimizer update statistic when they're used by query or when they might be outdated e.g. after insert\n",
    "    - _ASYNC asynchronously not at the same time\n",
    "    - AUTO_SHRINK when more than 25 % of files contains unused space, not recommended as db should grow\n",
    "    - RESTRICTED_USER is db owner / fixed db roles/ dbcreator/ sysadmin fixed server role\n",
    "    - RECOVERY model are for vm\n",
    "        - full use transaction log backup\n",
    "        - BULK_LOGGED minimal log for large scale\n",
    "        - simple only allows for complete full backup, differential backup/transaction log backup not possible, only for small db\n",
    "    - COMPATIBILITY_LEVEL available for mi and vm, each of number represent a version of sql server\n",
    "\n",
    "### create event notifcation\n",
    "- alert could be created from alert, metric or log tab in azure portal\n",
    "- how frequent the measure being group together and how frequent the alert rule being checked need to be selected when creating the rule, if grouped every 30 min, checking frequency can't be lower than every 30 min\n",
    "- static threshold would be based off of a fixed figure\n",
    "- dynamic threshold would be automatically decided by machine, taking into account factors like seasonality\n",
    "    - in dynamic mode how sensitive to the standard and how many number of violation within a arbitrary time period to trigger the alert, notice this should be a reachable goal with the checking frequency, default value is medium sensitive\n",
    "- then action and notification type could be selected for the alert\n",
    "- automatically resolve alert mean when value gets back to normal, resolve the alert automatically\n",
    "\n",
    "\n",
    "### source for performing metrics\n",
    "1. Azure tenant\n",
    "    - tenant wide service: azure active directory, \n",
    "2. azure activity log: include self service health record, configuration changes\n",
    "3. azure service health\n",
    "4. azure resources\n",
    "    - submit platform metrics to metrics db\n",
    "    - reource log then create internally regarding internal operation of resources\n",
    "    - for guest operating system, azure dignositc extension for vm if enabled or log analytic agent that can install on machine or vm insight\n",
    "\n",
    "\n",
    "### interpreting performace metrics\n",
    "- system dmdb resource stats: if any of that is close to 100 %, then upgrade the service plan might be needed, if too low downgrade can save money\n",
    "- xtp storage percent: percentage of storage used by In-Memory OLTP objects in Azure SQL Databas\n",
    "    - 0 if in memeory optimized table not used\n",
    "    - 100 update and insert will fail, select and delete are fine\n",
    "- max session percent: the max concurrent session allowed/service tier limit\n",
    "- max worker percent: max concurrent request/service tier limit\n",
    "- in intelleigence performance section in azure portal, in query performance insight and see the most heavy query by cpu/data io/ log io, some of them come with performance recommendation\n",
    "\n",
    "### intelligent insight\n",
    "\n",
    "- it compares current db workload to last 7 days performance\n",
    "- looks for things that could affect db performance e.g.resourcing limit, worklaod increase, memory pressure (workers waiting for memory allocation), data locking or need to increase maximum degree of parallelism = dop, missing index, new query affected performance, multiple thread using same temp db resource\n",
    "- automatically enable for azure sql db, need to manual enable for azure mi\n",
    "- ai detect high wait time/query prioritization/\n",
    "- not available for vm and some europe/us region\n",
    "- include \n",
    "    - query performance insight \n",
    "    - diagonostic setting\n",
    "        - log type includes\n",
    "            - SQLinsights\n",
    "            - automatic tuning\n",
    "            - timeouts\n",
    "        - destination, where the log can be sent to includes\n",
    "            - log analytics workspace\n",
    "            - storage account\n",
    "            - stream to event hub\n",
    "            - partner solution\n",
    "\n",
    "\n",
    "| event/ activity  | extended events | sql server profiler |distributed replay |system monitor = performance monitor | activity monitor in ssms| transact sql| error log|performance dashboard in ssms|\n",
    "| :----- | :------ | :------: |:-------: |:------: |:------: |:------: |:------: |-----: |\n",
    "| what it is  |   light weight monitoring system, using littele resource, allow create, modify, display, analyse session data   | track process event, monitor service and db activity |sql server profiler to replay on multiple machine, simulate a mission critical workload | track resource usage, monitor server performance using counters and rates| display info of process running on sql instance|   | window app event log (for vm) | identifying performance buttleneck in sql server|\n",
    "| trend analysis           | v | v |   | v |   |   |   |   |\n",
    "| replaying captured event |   | v(data can be save as sql table, replay step by step from a single computer) | v(data can be save as sql table, replay step by step from a multiple computer)a| | | | | |\n",
    "| adhoc monitoring         | v (xevent profiler)| v |   |   | v | v | v | v |\n",
    "| generating alarm         |   |   |   | v |   |   |   |   |\n",
    "| graphical interface      | v | v |   | v | v |   | v | v |\n",
    "| using within custom app  | v | v |   |   |   | v |   |   |\n",
    "\n",
    "### sql insight \n",
    "- retired at 2024 but still part of the exam\n",
    "- ms allows 1/more dedicated azure virtual machine (ubuntu 18.04, size b2s) to collect info from azure sql db, mi, vm then store it in log analytics space\n",
    "- sql insight/workbook template/log queries connects to this space, it's part of azure monitor\n",
    "- below s1, elastic pool, some price tier plans are not supported, bcs it constantly querying the db, its not suitable for serverless\n",
    "- require sql server authentication\n",
    "\n",
    "### db watcher\n",
    "- a centralized store for performance, configuration, health data for azure sql db & mi & elastic pool (* are also available for elastic pool)\n",
    "    - active history\n",
    "    - backup history\n",
    "    - change processing and error\n",
    "    - connectivity\n",
    "    - (mi: db)geo replica\n",
    "    - index metadata\n",
    "    - memory utilization *  \n",
    "    - missing index\n",
    "    - out of memory event *\n",
    "    - performance counter *\n",
    "    - db and instance property *\n",
    "    - query runtime and wait statistic\n",
    "    - db replica *\n",
    "    - resource utilization\n",
    "    - session statistic\n",
    "    - sos schedueler *\n",
    "    - sql agent job can only be stored on mi\n",
    "    - storage io *\n",
    "    - storage utilization *\n",
    "    - table metadata\n",
    "- it gets data from db and store it in :\n",
    "    - azure data explorer cluster: highly scalable data service for fast input and analytic \n",
    "    - microsoft fabric: realtime analytic \n",
    "- visualization is free, storage is not\n",
    "- once store can be query using kusto kql or tsql in azure data explorer dashboards, powerbi, gafana, excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef69cf4",
   "metadata": {},
   "source": [
    "# lesson 13: manage performance related maintainence task\n",
    "### index maintainence task\n",
    "\n",
    "- select * from sys.dm_db_missing_index_details \n",
    "\n",
    "- select * from sys.dm_db_physical_stats \n",
    "    -access fragmentation of db index\n",
    "- select * from sys.dm_db_column_store_row_group_physical_stats \n",
    "    -access columnstore index\n",
    "    - deleted row / total row = how fragmented index is\n",
    "    - when a row in column stored get updated, it is marked as delete\n",
    "\n",
    "-   ALTER INDEX ALL \n",
    "    ON [SalesLT].[Address] \n",
    "    REORGANIZE \n",
    "    - Reorganize indexes for fragmentation between 10-30 %, columstore for above 20&\n",
    "\n",
    "-   ALTER INDEX [PK_Customer_CustomerID] \n",
    "    ON [SalesLT].[Customer] \n",
    "    REBUILD WITH (ONLINE = ON,  \n",
    "    FILLFACTOR = 60, \n",
    "    MAX_DURATION = 30, \n",
    "    RESUMABLE = ON) \n",
    "\n",
    "    - Rebuild indexes  for fragmentation above 30 %\n",
    "    - fullfactor allow create 135 on the page (60% filled), and leave 2 and 4 for later, the index will be bigger due to blank page\n",
    "    - max_duration = time limit \n",
    "\n",
    " -  ALTER INDEX [PK_Customer_CustomerID] \n",
    "    ON [SalesLT].[Customer] \n",
    "    PAUSE/ABORT /RESUME \n",
    "    - pause/abort/resume an alter\n",
    "\n",
    "### statistic maintainence task\n",
    "- use to create query plan to improve the speed, estimate cardinality(# of rows), allow seek instead of scan for query plan\n",
    "- statistic includes dist of the value in table/indexed view col\n",
    "- query optimizer determine update automatically, but it can also be done manually, especially if:\n",
    "    - query execution time is slow\n",
    "    - inserting on ascending/descending key col (e.g.identity/timestamp)\n",
    "    - after maintainence operation e.g. bulk insert\n",
    "- not needed to update after rebuild/reorg as rows are not changed\n",
    "- EXEC sp_updatestats  \n",
    "    - Update a particular table or indexed view \n",
    "-   UPDATE STATISTICS [SalesLT].[Address] [AK_Address_rowguid] \n",
    "    WITH FULLSCAN/ SAMPLE 10 PERCENT(, PERSIST_SAMPLE_PERCENT = ON) / RESAMPLE (, PERSIST_SAMPLE_PERCENT = ON) \n",
    "    - with full scan scan all rows\n",
    "    - SAMPLE 10 PERCENT scan a sample\n",
    "    - RESAMPLE most recent sample rate\n",
    "    - PERSIST_SAMPLE_PERCENT set the value as default\n",
    "\n",
    "### db auto tuning\n",
    "- it's a process of learning about workload and identidy issue and improvement using: learn/adapt/verify/repeat\n",
    "- can be configure in intellegince insight in azure portal\n",
    "- for db the below 3 options are available, for mi only force plan, these setting could be inherited from server, multiple on possible\n",
    "    1. force plan=force_last_good_plan, enabled by default, if estimated gain above 10s or error of new plan is above tolerence then last good plan will be used\n",
    "    2. create index: auto create when cpu data io & log io are below 80 %, if performance are'nt improved, it'll be auto drop\n",
    "    3. drop index: not compatiable with partition switch & index hint\n",
    "-   SELECT * FROM sys.indexes  \n",
    "    WHERE auto_created = 1 \n",
    "    - show all index in db that's auto created\n",
    " \n",
    " -  ALTER DATABASE [DP300] SET AUTOMATIC_TUNING = AUTO --| INHERIT | CUSTOM \n",
    "    ALTER DATABASE [DP300] SET AUTOMATIC_TUNING (FORCE_LAST_GOOD_PLAN = ON, \n",
    "                                                CREATE_INDEX = ON, DROP_INDEX = OFF) \n",
    "    - Change database auto-tuning using tsql\n",
    "- SELECT * FROM sys.dm_db_tuning_recommendations \n",
    "\n",
    "\n",
    "### manage storage capcity for azure sql db (may or may not applied to mi)\n",
    "- data space used increase with insert, decrease with delete\n",
    "- data space allocated is space made available for data, increase with data space used , but not decrease with delete\n",
    "- SELECT database_name, allocated_storage_in_megabytes FROM sys.resource_stats \n",
    "    - to check allocated space for a saingle db in masterdb\n",
    "- SELECT elastic_pool_name, elastic_pool_storage_limit_mb, avg_allocated_storage_percent \n",
    "FROM sys.elastic_pool_resource_stats \n",
    "    - to check allocated space for an elastic pool in masterdb\n",
    "- SELECT DATABASEPROPERTYEX('DP300', 'MaxSizeInBytes') \n",
    "    - Display maximum size \n",
    "\n",
    "\n",
    "-   SELECT file_id, type_desc, size, max_size, growth \n",
    "    FROM sys.database_files \n",
    "    WHERE type = 1 \n",
    "    - View current log size \n",
    "    - if size = 1000, 1000 refers to page that are still free, for each page there are 8 kilobytes, so the total available space is 8000 kilobytes\n",
    "-   DBCC SHRINKFILE(file_id, megabyte_to_shrink_to) \n",
    "    DBCC SHRINKDATABASE(DP300) \n",
    "    - To shrink a transaction log file \n",
    "    - file_id megabyte_to_shrink_to are int\n",
    "    - dbcc = database console command\n",
    "- shrinking shouldn't be done too often as file needs to grow(?)\n",
    "\n",
    "### assess growth/fragmentation of db/log\n",
    "-   SELECT database_name, start_time, storage_in_megabytes \n",
    "\n",
    "    FROM sys.resource_stats  \n",
    "\n",
    "    ORDER BY database_name, start_time\n",
    "    -  Assess growth in a database for 1 database (need to be in the \"Master\" database)\n",
    "\n",
    "-   SELECT start_time, elastic_pool_name, elastic_pool_storage_limit_mb, \n",
    "    \n",
    "    avg_allocated_storage_percent  \n",
    "    \n",
    "    FROM sys.elastic_pool_resource_stats \n",
    "    \n",
    "    ORDER BY start_time  \n",
    "\n",
    "    - For an elastic pool (need to be in the \"Master\" database) \n",
    "\n",
    "- EXEC sp_spaceused \n",
    "    - Report on database free space \n",
    "\n",
    "-   SELECT allocated_extent_page_count, unallocated_extent_page_count  \n",
    "\n",
    "    FROM sys.dm_db_file_space_usage \n",
    "\n",
    "    - View number of pages \n",
    "- DBCC SQLPERF (LOGSPACE) \n",
    "    - transaction log space statistic\n",
    "\n",
    "- for vm in ssms disc usage can be directly viewed\n",
    "- SELECT * FROM sys.dm_db_session_space_usage \n",
    "    - show # of pages for each session, internal means tempdb\n",
    "\n",
    "- SELECT * FROM sys.dm_db_task_space_usage \n",
    "    - show # of pages for each task, internal means tempdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd421e0b",
   "metadata": {},
   "source": [
    "# lesson 14: identify performance related issue\n",
    "\n",
    "### query store\n",
    "- query that are regressed due to change in execution plan could be fixed\n",
    "- it's disabled by default on new sql db, on-perm , vm, enabled by defualt on azure sql db\n",
    "- statistic collection interval can be changed, first time activation might takes day until insight coulld be produced, too often collection might affect performance\n",
    "- default allocation for query store is 100 mb - 1 gb depend on sql server version\n",
    "- if limit being hit, query store will not collect new data and turn into read only mode\n",
    "- clean up can be automatically start when storage reach a certain percentage, it'll remove oldest/least expensive query data util back to another certain percentage\n",
    "- query store capture mode, default is auto (ignore infrequent/small/quick executed query), but can be changed to all or none\n",
    "- ALTER DATABASE [DP300] SET QUERY_STORE CLEAR\n",
    "    - remove query store, can also be done in ui in ssms\n",
    "- SELECT * FROM sys.query_store_plan  \n",
    "    - extract query plan\n",
    "\n",
    "-    SELECT Txt.query_text_id, Txt.query_sql_text, Qry.* \n",
    "\n",
    "    FROM sys.query_store_query AS Qry \n",
    "\n",
    "    INNER JOIN sys.query_store_query_text AS Txt \n",
    "\n",
    "    ON Qry.query_text_id = Txt.query_text_id \n",
    "\n",
    "    -  See the queries\n",
    "\n",
    "\n",
    "1. plan store\n",
    "    - for executing plan data\n",
    "\n",
    "2. runtime store\n",
    "    - for execution statistic data\n",
    "3. waits stats store\n",
    "\n",
    "- in ssms you could view regress query and seeduration, cpu time, logical read, physical read etc, in this view you could also set a particular plan for a specific query in future\n",
    "- in ssms you could view query with top overall resource consumption, the view here could be customized whihc stats to see and how far back it should be\n",
    "- in ssms track query you could give the query id and see and see the stats, in the view circle means completed, square means cancelled by client, triangle means failed by exception\n",
    "\n",
    "### identify session causing blocking\n",
    "- if an update is running and the the power is down halfway through, after power back on the computer will rollback the change, there is no half-complete for transaction, only success/fail, this is a lock\n",
    "- explicit transaction: when tsql state begin transaction, comit/rollback transaction, before end the session, the resource is locked\n",
    "- implicit transaction: skip above only write etl\n",
    "- blocking happen when a session wants to work on a resource that is accessing by other session, e.g. 2 updates on the same table at the same time\n",
    "- blocking is caused by poor transactional design/ long running transaction\n",
    "- lock type:\n",
    "    1. row lock: lock at individual row level\n",
    "    2. page lock: lock at page level (a page is 8192 characters)\n",
    "    3. entire table lock:\n",
    "- SELECT * FROM sys.dm_tran_locks \n",
    "    - To view locks: request mode show what the second accessor wants to do\n",
    "        - S: select\n",
    "        - X: exclusive \n",
    "        - IX: intent exclusive\n",
    "        - u: update\n",
    "-   SELECT session_id, blocking_session_id, start_time, status, command, DB_NAME(database_id) as [database], wait_type, wait_resource, wait_time, open_transaction_count \n",
    "\n",
    "    FROM sys.dm_exec_requests \n",
    "\n",
    "    WHERE blocking_session_id > 0; \n",
    "    - To view blocking: \n",
    "\n",
    "\n",
    "### isolation level\n",
    "-   SET TRANSACTION ISOLATION LEVEL  \n",
    "\n",
    "    READ UNCOMMITTED/READ COMMITTED/REPEATABLE READ/SNAPSHOT/SERIALIZABLE \n",
    "    - READ UNCOMMITTED: completely ignore blocking, might cause dirty read(reading data that is not commited and could be rolled back)\n",
    "    - READ COMMITTED:block depend on READ_COMMITTED_SNAPSHOT setting, can only read commited data\n",
    "    - REPEATABLE READ:read remains the same until the end of the same transaction, no block, if:\n",
    "        1. DB is not in recovery stats (restoring from backup)\n",
    "        2. if having another alter db: ALTER DATABASE [DP300] SET ALLOW_SNAPSHOT_ISOLATION ON \n",
    "        - if insert has been done,  REPEATABLE return new data\n",
    "\n",
    "    - SNAPSHOT: read remains the same until the end of the same transaction, no block\n",
    "        - if insert has been done,  SNAPSHOT doesn't return new data,  only expanded version would be generated\n",
    "    - SERIALIZABLE:block any dirty read, can't read stuff that is modified\n",
    "         - block update and insert but not read\n",
    "         - strictest, used in bank\n",
    "\n",
    "\n",
    "\n",
    "- ALTER DATABASE [DP300] SET READ_COMMITTED_SNAPSHOT ON \n",
    "    - off is default for vm, on perm, when off read_commited is blocked\n",
    "    - on is default for azure sql db, DML statement start generating row version that doesn't block read_commited \n",
    "- DBCC USEROPTIONS \n",
    "    - to see the current option\n",
    "\n",
    "| Isolation level   | Dirty read | nonrepeatable read | phantom|\n",
    "| :---------------- | :--------: |  :---------------: |:-----: |\n",
    "| read uncommitted  |      v     |          v         |   v    |\n",
    "| read committed    |            |          v         |   v    |\n",
    "| repeated read     |            |                    |   v    |\n",
    "| snapshot          |            |                    |        |\n",
    "| serealizable      |            |                    |        |\n",
    "\n",
    "- A phantom read occurs when a transaction performs a query with a WHERE clause (e.g., SELECT * FROM orders WHERE value > 1000), and then a second transaction inserts new rows that match the WHERE clause and commits. If the first transaction re-executes the same query, it might see more rows than it did initially. The original rows it read are still the same (repeatable read), but the set of rows has changed\n",
    "    - a transaction end when it's explictly commited/rolled back/ client connection closed etc.\n",
    "- Non-Repeatable Read: existing row always return same value\n",
    "\n",
    "- Phantom Read: new rows might come in\n",
    "\n",
    "### performance related db configuration\n",
    "- this could be done in UI of ssms or tsql\n",
    "- auto close: close when there is no connection , can't be enable in azure sql db\n",
    "- auto_create_statistic: default on, allow generation info of content of each column, good for optimizer to decide for scan or seek\n",
    "    -ALTER DATABASE [DP300]  SET AUTO_CREATE_STATISTICS OFF GO \n",
    "- auto_create_incremental_statistic: default off, collect even more thorough data than auto_create_statistic\n",
    "- auto_shrink: default off, less burden to performance compared to auto_create_statistic but also less effective, not recommanded\n",
    "\n",
    "- ``` \n",
    "    ALTER DATABASE SCOPED CONFIGURATION  \n",
    "    -- [FOR SECONDARY] \n",
    "    SET GLOBAL_TEMPORARY_TABLE_AUTO_DROP = ON \n",
    "    -- LAST_QUERY_PLAN_STATS \n",
    "    -- LEGACY_CARDINALITY_ESTIMATION \n",
    "    -- MAXDOP \n",
    "    -- OPTIMIZE_FOR_AD_HOC_WORKLOADS \n",
    "    -- PARAMETER_SNIFFING \n",
    "    -- QUERY_OPTIMIZER_HOTFIXES \n",
    "    GO \n",
    "    ```\n",
    "    - [FOR SECONDARY] if there is a geo-replicated db\n",
    "    - GLOBAL_TEMPORARY_TABLE_AUTO_DROP drop GLOBAL_TEMPORARY_TABLE if not used in any session\n",
    "    - LAST_QUERY_PLAN_STATS actual execution plan being shown or not in sys.db_exec_PLAN_STATS\n",
    "    - LEGACY_CARDINALITY_ESTIMATION: how many row each value has in db, should only be used for compatibility purpose\n",
    "    - MAXDOP: inter-query parallelism, max # of parellel thread, too high of the number migh impact performance, default 8 is usually a suitable #\n",
    "    - OPTIMIZE_FOR_AD_HOC_WORKLOADS: do a compiled plan stub when a batch is compliled for first time, by the second time, only when compile second time a full compiled plan will be used, this reduce memory footprint\n",
    "    - PARAMETER_SNIFFING : evluate stored procedure to create an execution plan, stick to the plan even parameter of stored procedure change, performance might be suboptimal\n",
    "    - QUERY_OPTIMIZER_HOTFIXES: use the QUERY_OPTIMIZER_HOTFIXES ignoring compatibility\n",
    "\n",
    "### Configure Intelligent Query Processing (IQP) \n",
    "\n",
    "- IQP is a set of features designed to improve query performance\n",
    "- can be done using tsql or ssms ui\n",
    "- first iop were included in sql server 2017 ( interleaved execution tvf & batch mode memory grant feedback), 2019 added a lot more\n",
    " \n",
    "- SELECT * FROM sys.configurations \n",
    "    - Server-wide configuration options \n",
    "- EXEC sp_configure '101', 0\n",
    "    - changing server configuration in azure sql mi or vm, not in db\n",
    "- SELECT * FROM sys.database_scoped_configurations \n",
    "    - Database-wide configuration options\n",
    "- ALTER DATABASE SCOPED CONFIGURATION  SET [config_name] = ON \n",
    "    - changing db configuration \n",
    "    - approx_count_distinct can't be disabled\n",
    "- SELECT * FROM [schema].[table] OPTION (USE HINT ('config_name')) \n",
    "    - changing db configuration for one particular query\n",
    "\n",
    "\n",
    "- adaptive join (batch mode)\n",
    "    - it select between join or the nested looped join during runtime based on scanning actual first input rows\n",
    "    - requirement:\n",
    "        - col stored index in the query\n",
    "        - or a table being referenced in the join\n",
    "        - or batch mode enabled for row stored\n",
    "- approx_count_distinct\n",
    "    - approximation for aggragation of count distinct in each column, suitable for big data, decrease memory & performance requirement, 2 % error rate with 97% possibility\n",
    "    - can't be disabled\n",
    "- batch mode on row store\n",
    "    - queries can work on batches of rows instead of 1 row at a time when cached, reduce db workload\n",
    "    - happens by query plan decision, no need to set up\n",
    "- interleaved execution tvf\n",
    "    - it use the cardinality of multistatement table-valued-function (tvf) on first compilation instead of guessing, only for read only, e.g. select \n",
    "    - default fixed guess is 100 row\n",
    "- batch mode memory grant feedback\n",
    "    - there is also a row mode memory grant feedback\n",
    "    - sql server look for how much memory allocated to a cached query, for the second run allocated same amount of memory to it, alternative is guessing memor, then adjust memory grant next time based on usage\n",
    "- tsql scalar udf lining\n",
    "    - user defined function, usually performa badly bcs they might run multiple time once per row bcs unable to work out performance cost\n",
    "    - scalar udf transform into the equivalent related expression, it takes out a function put it into select (inlined) and increase performance\n",
    "    - doesn't work with all function, especially for function with multiple return \n",
    "    - can be disabled for specific udf\n",
    "- deffered compilation tv\n",
    "    - it use the cardinality of multistatement table variable (tv) on first compilation instead of guessing, only for read only, e.g. select \n",
    "    - default fixed guess is 1 row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0d982",
   "metadata": {},
   "source": [
    "# lesson 15: automatic task and performance backup\n",
    "\n",
    "### elastic job agent\n",
    "- automate db maintenance task\n",
    "- one could have a lot of db all over the place, some are azure sql db and some in the pool, with different server and subscription, for united actions on all these db (target group/ db of schard maps) e.g. backup, credential, collect performance data, telemetry data ( the automatic collection and transmission of data from remote sources), update reference data can be done through elastic job agent, as long as they are in the same azure cloud \n",
    "    - shard maps = how data is distributed across multiple databases (shards)\n",
    "- in mi the equivalent is sql server agent job\n",
    "- it charge for as a azure sql db, must be above basic tier, s0 can but isn't recommended, for frequent job even higher tier would be better\n",
    "- an elastic job db is needed for an agent, a blank or azure sql db could be used to store logs, meta data , job related data, stored procedure \n",
    "- elastic job can execute (regardless of db is created before job agent):\n",
    "    - against all db in a server\n",
    "    - all db in a pool\n",
    "    - single db\n",
    "    - exclude specific db in target\n",
    "- master db credential is needed to enumerate all db\n",
    "    -  enumerate = the process of sequentially accessing each database in a group\n",
    "- a job is a until of work which contains steps, each are tsql scripts and other details\n",
    "- script must be Idempotent \n",
    "    - idempotent = operations produce the same result even when the operation is repeated many times\n",
    "- then it'll generate output and job history, output could be saved as table, job history will be stored in job.job_executions for 45 days\n",
    "- creation step:\n",
    "    1. create azure sql db. minimum s0\n",
    "    2. go to azure portal using ui/powershell create elastic job agent\n",
    "    3. create db master key using tsql in ssms, password must be strong\n",
    "        - CREATE MASTER KEY ENCRYPTION BY PASSWORD='<an6?%9++Vyd%Ut9';\n",
    "    4. CREATE DATABASE SCOPED CREDENTIAL, need one to execute job,  one to refresh db metadata in the server\n",
    "        - CREATE DATABASE SCOPED CREDENTIAL [MasterCred] WITH IDENTITY = 'MasterU', SECRET = 'an6?%9++Vyd%Ut9'\n",
    "        - CREATE DATABASE SCOPED CREDENTIAL [RunJob] WITH IDENTITY = 'JobU', SECRET = 'an6?%9++Vyd%Ut9'\n",
    "    5. Create a target group \n",
    "        - EXEC jobs.sp_add_target_group 'GrpDatabase';\n",
    "    6. add a target group member \n",
    "\n",
    "        ´´´\n",
    "        EXEC jobs.sp_add_target_group_member \n",
    "        @target_group_name = 'GrpDatabase', \n",
    "        @target_type = 'SqlDatabase',   -- or 'SqlServer' or 'PoolGroup' \n",
    "        -- @membership_type = 'Exclude', \n",
    "        -- @refresh_credential_name = 'RefreshPassword', \n",
    "        @server_name = 'dp300database.database.windows.net', \n",
    "        @database_name = 'dp300'; \n",
    "        ´´´\n",
    "        - @membership_type = 'Exclude'  to exclude a certain computer\n",
    "        - @refresh_credential_name = 'RefreshPassword' when target_type is a server or pool\n",
    "    7. to create master user run this tsql In Master Database \n",
    "        ´´´\n",
    "        CREATE LOGIN MasterU WITH PASSWORD ='<an6?%9++Vyd%Ut9' \n",
    "        CREATE USER MasterU FROM LOGIN MasterU \n",
    "        CREATE LOGIN JobU WITH PASSWORD = '<an6?%9++Vyd%Ut9' \n",
    "        ´´´\n",
    "        - for the job to run we need 2 users, both with login in master db, one user in master db, one user in Target User db \n",
    "    8. to create user in Target User Database \n",
    "        ´´´\n",
    "        CREATE USER JobU FROM LOGIN JobU \n",
    "        ALTER ROLE db_owner ADD MEMBER JobU\n",
    "        ´´´\n",
    "    9. Create a job and job steps in elastic job agent db\n",
    "        ´´´\n",
    "        EXEC jobs.sp_add_job @job_name='My first job', \n",
    "        @description='Look at objects' \n",
    "        EXEC jobs.sp_add_jobstep @job_name='My first job', \n",
    "        @command='SELECT * FROM sys.objects', \n",
    "        @credential_name='RunJob', \n",
    "        @target_group_name='GrpDatabase' \n",
    "        ´´´\n",
    "    10. Run/schedule the job in T-SQL \n",
    "        - EXEC jobs.sp_start_job 'My first job' \n",
    "            - Run the job now\n",
    "\n",
    "        ´´´\n",
    "        EXEC jobs.sp_update_job \n",
    "        @job_name='My first job', \n",
    "        @enabled=1, \n",
    "        @schedule_interval_type='Minutes', -- Or Hours, Days, Weeks, Months or Once, \n",
    "        @schedule_interval_count=1 \n",
    "        ´´´\n",
    "            - schedule the job \n",
    "    11. Monitor job execution \n",
    "        - SELECT * FROM jobs.job_executions order by start_time \n",
    "        - or in azure portal seeing the last 100 job execution\n",
    "    - adding target groups/credentials can only be done through tsql/powershell/visual studio, but not in azure portal\n",
    "\n",
    "### log backup\n",
    "\n",
    "- in azure sql db backup is done automatically\n",
    "1. full backup \n",
    "    - is done every single week, same as in mi\n",
    "2. differential backup\n",
    "    - done every 12-24 hr, back up the current difference with last full backup\n",
    "3. transactional backup\n",
    "    - every 5 to 10 min backup everything since last backup(all 3 types)\n",
    "- restore to point in time in last 7 day (option 1 to 35, default 7, basic only 7) with these backups is possible\n",
    "- to restore to a point in time last full back up, last differential back up between that full backup and now, all transactional backup between that differential backup and now will be needed\n",
    "\n",
    "- azure portal and powershell can both carry out a recover\n",
    "- a new db is needed for restoration, restore over an existing db is not allowed\n",
    "- restore db usually need to be on the same server\n",
    "- to restore it to a different region, create a new db in the new region, in advanced using backup to populate the db\n",
    "\n",
    "### longterm backup retention (ltr)\n",
    "- available for azure sql mi and db\n",
    "- the functionality is in public preview\n",
    "- a backup for longer than 35 days\n",
    "- only control by azure not admin\n",
    "- might take up to 7 days before the first ltr backup\n",
    "- secondary db should have ltr configured, otherwise failed over from primary db will lead to lose of ltr\n",
    "- backups are stored in azure blob storage, a different storage container each week\n",
    "- in configuration backup length for weekly, monthly, and yearly can be selected, but not timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e15c3",
   "metadata": {},
   "source": [
    "# lesson 16: high availability and disaster recovery (HADR) strategy for data platform\n",
    "\n",
    "### service level agreement\n",
    "- service level agreement (SLA) is what ms minimum guarantee, if ms failed to meet it, they'll refund\n",
    "1. db\n",
    "    - for hyperscale there is a min uptime for 99,9 % if no replica db, 99,95 % if 1 replica\n",
    "    - other tiers  are 99.99 %replica\n",
    "    - if configured for zone redundant deployment(db that have multiple synchronized in diff building of same region) + business critical/premium then 99,995%\n",
    "2. mi\n",
    "    - 99.99% (four 9)\n",
    "3. vm\n",
    "    - 95% - 99,99%\n",
    "    - sql server on vm might fail on a healthy vm, so real sla is even lower for vm\n",
    "\n",
    "### HADR strategy based on RPO/RTO\n",
    "- geo-replica\n",
    "    - setting can be configured under replica \n",
    "    - primary db send out data through asynchronous replication to replicas/secondaries, the process is called seeding. the update is replicated asynchronously, which means they are committed to primary before to secondary\n",
    "    - secondary need to be at least same service tier as primary, while upgrading, upgrade secondary first then primary, only when changing from general to business critical need to disconnect the 2\n",
    "    - up tp 4 replicas possible\n",
    "    - building replica for replica is possible\n",
    "    - secondary can be read, so using primary for write and secondary for read can help sharing the workload\n",
    "    - after failure, one of the secondary can be appointed as new primary and the other secondary will be connected to this new primary, but the connection string will need to be updated manually\n",
    "- failover group\n",
    "    - with failover group, azure will manage the connection string for a group of db, once primary is failed, it switch to a secondary without changing the string\n",
    "    - if choosing automatic failover policy, manual failover will still be available\n",
    "|                                    | Geo replica | failover group |\n",
    "| :--------------------------------: | :-: | :-: |\n",
    "| automatic failover                 |  x  |  v  |\n",
    "| failover multiple db simultaneously|  x  |  v  |\n",
    "| sql mi support                     |  x  |  v  |\n",
    "| user update connection string after failover (time consuming) |  v   | x |\n",
    "| can be in same region as primary   |  v  |  x  |\n",
    "| multiple replica                   |  v  |  x  |\n",
    "| support read-scale                 |  v  |  v  |\n",
    "\n",
    "- recovery point objective (RPO) = how much data one could lose \n",
    "- recovery time objective (RTO) = max failover time, during rto no backup will happen\n",
    "- with a geo replication,\n",
    "    - manual db failover, rpo is 5 sec, rto is 30 sec\n",
    "    - auto failover group, rpo is 5 sec, rto is 1 hr minimum\n",
    "- for a geo-restore from geo replicated backup, rpo is 1 hr, rto is 12 hr\n",
    "- the shorter the fail timeframe, the more expensive it cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb4d83",
   "metadata": {},
   "source": [
    "# lesson 18 & 5 intro to vm and mi\n",
    "### review of lesson 5 deployment method\n",
    "\n",
    "| Azure SQL DB      | mi| vm |\n",
    "| :------: | :------: | :------: |\n",
    "| platform as a service       |   platform as a service   | infrastructure as a service |\n",
    "| Azure managed DB           |   Azure managed DB   | user self manage |\n",
    "| always running unless dropped(for serverless unless paused)    |  always running unless dropped(for serverless unless paused)   | can be shut down when not used, when shut down there is a storage cost |\n",
    "|   |     | more expensive |\n",
    "| best for modern cloud solution & fast time to market  |  best for new or in-prem application to use in/migration to cloud   | lift and shift, can be easily moved from on-prem server to another |\n",
    "|  SQL compatibility best |   SQL compatibility second  | best when don't want any change/require OS level service |\n",
    "|  most commonly used sql server feature |  high compatability with sql server   | all on-premise compatable |\n",
    "|  trace flag aren't supported |   some trace flag are supported  | all trace flag are supported |\n",
    "| support serverless & provisioned compute  |     | all on-premise compatable |\n",
    "| .net framework, common language runtime not supported  |   .net framework, common language runtime supported  |  |\n",
    "|  using latest SQL server version |   using latest SQL server version  | using any version of SQL server 2008r2 onwards |\n",
    "|  can use elastic job agent service |   can use SQL agent job, no need of elastic pool bcs everything are already in a pool  |can use SQL agent job  |\n",
    "|  CLR supported |     CLR not supported  |  |\n",
    "\n",
    "**note** trace flags in SQL Server are specialized configuration settings that alter the behavior of the database engine. They are used to enable or disable specific features, diagnose issues, or fine-tune performance, often providing access to undocumented or advanced options.\n",
    "\n",
    "**note** all 3 can use Azure hybrid benefit (windows server for VM, SQL server license for software assurance) & reserve capcity to reduce cost, but hybrid benefit doesn't apply to serverless azure sql db\n",
    "\n",
    "### scalability & security\n",
    "| Azure SQL DB      | mi| vm |\n",
    "| :------: | :------: | :------: |\n",
    "|  storage up to 100tb |  general purpose up to 8 tb,business critical 1-4 tb. up to 100db, next-gen general purpose up to 500db. subject to number of vcores  | instance up to 256tb, up to 50 instance per server (db of up to instance size) |\n",
    "| size of db/elastic pool can be changed  |  size  can be changed   | size of vm can be changed |\n",
    "|  vertical scaling and horizontal scaling allowed |  vertical scaling allowed, horizontal scaling not easy   | adding more compute power to VM is possible |\n",
    "|  service tier can be changed from standard/general purpose (premium disc) to premium(premium disc)/business critical(ssds) |   service tier can be changed between premium/business critical  |  |\n",
    "|  downgrade is not allowed, only can be decommissioned |     |  |\n",
    "|  auditing work at db level |  auditing work at server level   | auditing work at server level |\n",
    "|  .xel log file are stored in Azure Blob storage |  .xel log file are stored in Azure Blob storage   | event stored in window events log/file system |\n",
    "|  authentication:SQL server /Azure active directory |  authentication:SQL server /Azure active directory   | authentication:SQL server /windows |\n",
    "|   |     |  |\n",
    "\n",
    "**note** vertical scaling = adding more compute power\n",
    "**note** horizontal scaling = sharing data into multiple database nodes\n",
    "**note**  can use azure defender (include vuneraility assessment & threat detection) (0,02dollar/hr/instance)\n",
    "**note** data encryption, can use transport layer security (tls)/transport data encryption (tds)/always encrypted & firewall\n",
    "**note** local replica is replicated within same storage building (shared power/network/cooling), zone replica will be at a seperate geo location, so if a natural disaster hit it might destroy all local replica, but zone replica should be safe, geo replica make sure it's in another geo selection (e.g. US west to US central)\n",
    "\n",
    "### HADR\n",
    "| Azure SQL DB      | mi| vm |\n",
    "| :------: | :------: | :------: |\n",
    "|  Minimum of a 99,99% SLA availability, max 99,995%, hyperscale between 99,9-99,95% |  Minimum of a 99,99% SLA availability   | Minimum of a 95% SLA availability, could be increased to 99.99% through a secondary vm and always on availability group |\n",
    "| build in backup (7-35days), patching and recovery  |  same as db   |user manage backup, patching and recovery |\n",
    "|  for basic/standard/general purpose, local redundant availability can be used | same as db   |can configure availability replica (a place to switch to when a instance go down) using domain controller VM  |\n",
    "|  for premium/business critical/elastic pool, 3-4 node cluster with zone/local redundant availability can be used, also read only replica |   same as db  |  |\n",
    "|  can configure full db backups to azure storage for longterm backup retention (LTR) |  can do copy only backups to azure storage for longterm backup retention (LTR)   | can configure backup |\n",
    "|  with LTR we can do point in time retention which is reverse to a previous point in time |   same as db  | with appropriate backup can also do point in time restore |\n",
    "|  can configure up to 4 readable secondary db active geo-replica |  cannot   |  can configure geo-replica but not sync(async), data file and log need to be on same disc|\n",
    "| can configure auto-failover group, if db a fail use db b, if still fail then db c  |  same as db  |  can configure azure failover cluster instance using shared storage|\n",
    "|  at general purpose with gen5 hardware in some regions can use zone redundant config  |     |  |\n",
    "\n",
    "\n",
    "### others about mi\n",
    "- commen language runtime (CLR) is the execution enviroment for .net framework code e.g. c#, vb.net\n",
    "- .net code is also named as manage code \n",
    "- mi have msdb system db\n",
    "- manual copy of backup of db is possible, but not mi\n",
    "- it doesn't support dtu based purchasing model\n",
    "- mi deploy a dedicated ring field data = virtual cluster = a set of service component hosted on dedicated set of isolated virtual machine that run inside virtual network subnet\n",
    "- a virtual cluster can host multiple mi\n",
    "- mi create 12 files regardless of vcode #\n",
    "- tempdb is available in mi\n",
    "\n",
    "skip the rest \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798db6e6",
   "metadata": {},
   "source": [
    "# lesson 19 configure db authentication \n",
    "### Create users from Azure AD identities\n",
    "- for azure sql db \n",
    "    - CREATE user username FROM EXTERNAL PROVIDER \n",
    "- for mi vm\n",
    "    - \n",
    "    ```\n",
    "    CREATE LOGIN MyLogin    \n",
    "    WITH PASSWORD = 'MyComplexPassword';  \n",
    "    CREATE USER MyLogin FOR LOGIN MyLogin;\n",
    "    ```\n",
    "    - login will be created in master db (login should be run in master db) and user in other db\n",
    "- create user from azure active directory identity\n",
    "    - first need to connect mi to azure active directory identity in azure portal\n",
    "    - \n",
    "    ```\n",
    "    CREATE LOGIN [Susan@Filecats.onmicrosoft.com] \n",
    "    FROM EXTERNAL PROVIDER  -- DEFAULT_DATABASE =  -- DEFAULT_LANGUAGE =  \n",
    "    CREATE USER [Susan@Filecats.onmicrosoft.com] \n",
    "    FOR LOGIN [Susan@Filecats.onmicrosoft.com] \n",
    "    ```\n",
    "- vm need to flip setting to connect to azure active directory identity\n",
    "- vm cannot use FROM EXTERNAL PROVIDER \n",
    "- SELECT * FROM sys.server_principals \n",
    "    - checking server priciple\n",
    "\n",
    "### manage certificate\n",
    "- Create a self-signed certificate\n",
    "    - \n",
    "    ``` CREATE CERTIFICATE CertificateName    \n",
    "    ENCRYPTION BY PASSWORD = 'ComplicatedPassw0rd!' \n",
    "    WITH SUBJECT = 'CertificateSubjectName', \n",
    "    EXPIRY_DATE = '20291231';   \n",
    "    GO \n",
    "    ``` \n",
    "    - date is utc/gmt\n",
    "    - wo ENCRYPTION BY PASSWORD = 'ComplicatedPassw0rd!'  it'll be ENCRYPTION by dn master key\n",
    "    - start date default to current date wo specified\n",
    "    - EXPIRY_DATE default 1 year after start date\n",
    "- delete\n",
    "``` \n",
    "DROP CERTIFICATE CertificateName \n",
    "GO \n",
    "```\n",
    "- Creates certificate with existing private key \n",
    "    - \n",
    "    ```\n",
    "    CREATE CERTIFICATE CertificateName2    \n",
    "    FROM FILE = 'C:\\Certs\\Certificate.cer'    \n",
    "    WITH PRIVATE KEY (FILE = 'C:\\Certs\\Certificate\\PrivateKey.pvk',    \n",
    "    DECRYPTION BY PASSWORD = 'ComplicatedPassw0rd!2');   \n",
    "    GO \n",
    "    ```\n",
    "    - not supported by azure sql db, only in mi/vm\n",
    "- Alters/removes/imports private key \n",
    "    - \n",
    "    ```\n",
    "    ALTER CERTIFICATE CertificateName2 \n",
    "    ```\n",
    "    - subject can only be changed through drop and recreate\n",
    "- REMOVE PRIVATE KEY \n",
    "    - \n",
    "    ```\n",
    "    WITH PRIVATE KEY (FILE = 'C:\\Certs\\Certificate\\PrivateKey2.pvk',    \n",
    "    DECRYPTION BY PASSWORD = 'ComplicatedPassw0rd!3');\n",
    "    ```\n",
    "- Creates certificate from assembly (DLL file) \n",
    "    - \n",
    "    ```\n",
    "    CREATE CERTIFICATE CertificateName3   \n",
    "    FROM EXECUTABLE FILE = 'C:\\Certs\\Certificate\\Assembly.dll';   \n",
    "    GO\n",
    "    ```\n",
    "\n",
    "###  Configure security principals (MI and VM) \n",
    "- roles:\n",
    "    - sys admin: all activity\n",
    "    - server admin: change serverwide config/shut down server\n",
    "    - security admin: grant/deny server/db level permission\n",
    "    - process admin: end process\n",
    "    - setupadmin: allow add/remove linked server, if linked, cross server select possible\n",
    "    - bulkadmin: run bulk insert\n",
    "    - discadmin: manage disc file\n",
    "    - dbcreator: create/drop/restore db, only for mi & vm, in azure sql db it's dbmanager\n",
    "- viewing settings\n",
    "    ```\n",
    "    SELECT * FROM sys.server_principals \n",
    "    SELECT * FROM sys.sql_logins \n",
    "    SELECT * FROM sys.login_token \n",
    "    ```\n",
    "- add user\n",
    "    ```\n",
    "    ALTER SERVER ROLE [diskadmin] \n",
    "    ADD MEMBER [Susan@Filecats.onmicrosoft.com] \n",
    "    ```\n",
    "- show user permission for each object in db, all role, direct member of each role \n",
    "    ```\n",
    "    EXEC sp_helprotect \n",
    "    EXEC sp_helprole \n",
    "    EXEC sp_helprolemember \n",
    "    ```\n",
    "\n",
    "- create and grant access to new role. need to be in master\n",
    "    ```\n",
    "    CREATE SERVER ROLE newrole AUTHORIZATION [Jane@Filecats.onmicrosoft.com] \n",
    "    GO \n",
    "    ALTER SERVER ROLE newrole \n",
    "    ADD MEMBER [Susan@Filecats.onmicrosoft.com] \n",
    "    GO \n",
    "    USE master \n",
    "    GO \n",
    "    GRANT ALTER ON LOGIN::[MyLogin] TO [newrole] \n",
    "    GO\n",
    "    ```\n",
    "- give user select right on table\n",
    "    - grant select on object::dbo.tablename to username\n",
    "\n",
    "### table index storage in regard of file groups for mi & vm\n",
    "- azure sql db support only 1 db file except for hyperscale\n",
    "- in db property there are:\n",
    "    1. .mdf is primary file with start up info, only 1 per db\n",
    "    2. secondary file is unlimited and can be user defined, not allowed in azure sql db\n",
    "        - recommended format is .ndf\n",
    "        - can be located on different disc\n",
    "    3. transactional log, recommended ending is .ldf\n",
    "    - m stands for main, n come after m, so it stands for secondary, l stands for log\n",
    "    - one file can only be use in one db\n",
    "    - a simple db will have only 1 & 3\n",
    "    - logical name of file will be used in tsql \n",
    "    - operation system name is location including its directory path\n",
    "    - in mi location of file is out of control, in vm user can decide\n",
    "    - file size can be restricted\n",
    "    - primary file group is the default file group\n",
    "    - a file can only be contain in 1 file group\n",
    "    - a file group can only be used in 1 db\n",
    "    - transactional log are not part of a file group\n",
    "    - when multiple file are created, they need to be grouped and set as primary file group\n",
    "- changes of the files can be done through ui/tsql\n",
    "- add new file group \n",
    "    ```\n",
    "    ALTER DATABASE [dp300mia] \n",
    "    ADD FILEGROUP [NewFileGroup] \n",
    "    GO \n",
    "    ```\n",
    "- add file to file group\n",
    "    ```\n",
    "    ALTER DATABASE [dp300mia]  \n",
    "    ADD FILE (NAME = N'NewData2',  \n",
    "    -- FILENAME = N'C:\\PathToData\\NewData.ndf' , -- this is for VMs \n",
    "    SIZE = 8192KB , \n",
    "    FILEGROWTH = 65536KB )  -- or FILEGROWTH = 10% \n",
    "    TO FILEGROUP [NewFileGroup] \n",
    "    GO \n",
    "    ```\n",
    "- AUTOGROW_ALL_FILES,if any file in the group hit the threshold, then all files will have increaced limit \n",
    "    ```\n",
    "    ALTER DATABASE [dp300mia]  \n",
    "    MODIFY FILEGROUP [NewFileGroup] \n",
    "    AUTOGROW_ALL_FILES -- Add a second filegroup \n",
    "    ```\n",
    "- to see where a particular object is\n",
    "    - EXEC sp_help 'dbo.myTable' \n",
    "- create table in a specific file group, if not specified, it'll locate in default file group\n",
    "    ```\n",
    "    CREATE TABLE myTable2 \n",
    "    (intmyTable2 INT PRIMARY KEY IDENTITY(1,1)) \n",
    "    ON [NewFileGroup] \n",
    "    ```\n",
    "- create index on a specific file group\n",
    "    ```\n",
    "    CREATE NONCLUSTERED INDEX idx_myTable2 ON dbo.myTable2 (intmyTable2) \n",
    "    ON [NewFileGroup] \n",
    "    ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b67915",
   "metadata": {},
   "source": [
    "# lesson 20: evaluate and implement alarm for vm/mi\n",
    "\n",
    "### create event notification based on metric\n",
    "- in vm go into sql agent in ssms and create it through ui\n",
    "- type sql server event based on error number/ severity\n",
    "- type wmi event alert use windows management instrumentation to monitor event in sql server\n",
    "- type sql server performance condition allow alert based on object/statistic\n",
    "- when a alert is triggered, a job (e.g. a tsql file) or notification can be conduct\n",
    "### configure notification\n",
    "- a db mail account for sql server agent should be created\n",
    "- a db mail profile for sql server agent should be created, user need to be add to databsemailuser role in msdb\n",
    "- the profile need to be set as the default profile in msdb\n",
    "- operator can be created in mi/vm, can be used with alert/jobs\n",
    "- operator is a person with contact details\n",
    "- in vm to send an email db mail need to be enabled, mi is enabled by default\n",
    "### manage schedule and automate jobs in sql server agent\n",
    "- job can be created in ssms ui\n",
    "- in job multiple steps are allowed, one step could be one tsql script/powershell/etc, and based on success/failure the step, next step will be executed\n",
    "- job can be one time/ recurring\n",
    "- in vm job could be start automatically when sql server agent start, on mi sql server agent  always run, so it's not recommended\n",
    "- after job is completed/failed, notification/ auto deletion are poaaible next steps\n",
    "- page things should be deprecated\n",
    "- the same can be done in tsql, ui will translate your selections into a script\n",
    "- tsql need to be run in msdb db that's where all schedules are stored\n",
    "- job could be run daily/weekly/monthly, hourly is not an option\n",
    "### create alert for server config \n",
    "- to view the change history in mi, right click instance report, standard report, configuration change history\n",
    "- make config of advanced option viewable\n",
    "    ```\n",
    "    sp_configure 'show advanced options', 1 \n",
    "    GO \n",
    "    RECONFIGURE \n",
    "    GO \n",
    "    ```\n",
    "- enable config trace\n",
    "    ```\n",
    "    sp_configure 'default trace enabled', 1 \n",
    "    GO \n",
    "    RECONFIGURE \n",
    "    GO\n",
    "    ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7999a",
   "metadata": {},
   "source": [
    "# lesson 21 performance related issue for vm/mi\n",
    "\n",
    "### extra sources for performance metrics for vm\n",
    "1. performance monitor\n",
    "    - vm has more metrics\n",
    "2. vm insight\n",
    "    - not enabled by default \n",
    "    - not free, charge by ingested data\n",
    "    - can view performance/map/health\n",
    "\n",
    "### database engine tuning advisor (deta)\n",
    "- extra way to maintain index for vm\n",
    "- accessible through right click on query in ssms\n",
    "- before start analyzing query in deta, going to tuning, advanced, define maximum space for recommendation is needed\n",
    "\n",
    "### extra option for monitoring event for mi & vm\n",
    "1. blocking transaction \n",
    "    - in ssms right click on db, report, standard report,view all blocking transaction\n",
    "    - available for azure sql db as well \n",
    "2. extended event\n",
    "    - more light weight than sql profiler\n",
    "    - can trouble shoot blocking, dead locking performance issue, identifying long running query, monitoring ddl operation, logging missing column statistic, observing memory pressure, observing long running input/output\n",
    "    - it locates in management session, right click on start a new session to use\n",
    "    - in the session data storage:\n",
    "        - written buffer for smaller dataset/continuous data collection, asynchronous\n",
    "        - the event failed target for larger record\n",
    "    - after creation, the new session can be viewd in sessions, right click view live data\n",
    "3. sql profiler \n",
    "    - deprecated\n",
    "    - have graphical representation for dmv\n",
    "\n",
    "### resource governor\n",
    "- allows to categorized incoming request and decide for each category jpw much cpu/memory/networking it can draws\n",
    "- it's used in azure sql db but not configurable, only in mi/vm it is \n",
    "- Enable Resource Governor or through ui\n",
    "    - ALTER RESOURCE GOVERNOR RECONFIGURE\n",
    "- creation of all blow groups through ui is also possible\n",
    "- resource pool\n",
    "    - default: all new session are classified into default workload group\n",
    "    - default: all system request are classfied into internal workload group\n",
    "\n",
    "    - Create resource pools\n",
    "        ``` \n",
    "        CREATE RESOURCE POOL pDaytime \n",
    "        WITH (MAX_CPU_PERCENT = 20); \n",
    "            -- MIN_/MAX_CPU_PERCENT \n",
    "            -- CAP_CPU_PERECENT \n",
    "            -- MIN_/MAX_MEMORY_PERCENT \n",
    "            -- MIN_/MAX_IOPS_PER_VOLUME \n",
    "        GO  \n",
    "        CREATE RESOURCE POOL pNighttime \n",
    "        WITH (MAX_CPU_PERCENT = 50); \n",
    "        GO  \n",
    "        ALTER RESOURCE GOVERNOR RECONFIGURE;  \n",
    "        ```\n",
    "- workload group\n",
    "    - a container for request\n",
    "    - having 1 resource pool attached to it\n",
    "    - multiple workload group can be attached to same resource group\n",
    "    - setting includes: cpu/memory/max request/degree of parallelism/time out\n",
    "    - Create workload groups \n",
    "        ```\n",
    "        CREATE WORKLOAD GROUP gDaytime \n",
    "        USING pDaytime;   \n",
    "        GO \n",
    "        CREATE WORKLOAD GROUP gNighttime \n",
    "        USING pNighttime;   \n",
    "        GO \n",
    "        ALTER RESOURCE GOVERNOR RECONFIGURE;  \n",
    "        GO \n",
    "        ```\n",
    "- function classifier\n",
    "    - this is the criteria define which request go to which workload group\n",
    "    - Create Classifier Function \n",
    "        ```\n",
    "        USE master \n",
    "        GO \n",
    "        CREATE FUNCTION ClassifierFunction() \n",
    "        RETURNS sysname \n",
    "        WITH SCHEMABINDING \n",
    "        AS \n",
    "        BEGIN \n",
    "        if DATEPART(HOUR,GETDATE())<8 or DATEPART(HOUR,GETDATE())>17 \n",
    "        BEGIN \n",
    "        RETURN 'gNighttime'; \n",
    "        END \n",
    "        RETURN 'gDaytime'; \n",
    "        END \n",
    "        ```\n",
    "    - Register this classified function, associate the function with RESOURCE GOVERNOR\n",
    "        ```\n",
    "        ALTER RESOURCE GOVERNOR with (CLASSIFIER_FUNCTION = dbo.ClassifierFunction);   \n",
    "        ALTER RESOURCE GOVERNOR RECONFIGURE;   \n",
    "        GO \n",
    "        ```\n",
    "\n",
    "- DMVs\n",
    "    ```\n",
    "    SELECT * FROM sys.resource_governor_configuration \n",
    "    SELECT * FROM sys.dm_resource_governor_resource_pools \n",
    "    SELECT * FROM sys.dm_resource_governor_workload_groups \n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd763ed",
   "metadata": {},
   "source": [
    "# lesson 22: create scheduled task for vm/mi\n",
    "\n",
    "### apply patch & updates for hybrid & iaas(vm) deployment\n",
    "- in vm there is a linux/windows background, and these os need to be updated, sql sometime needs to be patched\n",
    "- in portal, sql server configuration, can access such info, but vm must be running\n",
    "- automated patched up can be setup when creating the machine\n",
    "- once patching is enable, setting maintenance schedule/timing/ will become possible, the same setting can also be done through powershell\n",
    "\n",
    "### implement disc encryption for azure vm\n",
    "- disc encryption help with preventing file stealing just from just having access \n",
    "- disc encryption could be done in portal, disk, additional setting\n",
    "- to encrypt the disc, functioning azure key vault is needed\n",
    "\n",
    "### configure multi server automation\n",
    "- in mi/vm, there could be 1 master server, where job is defined and multiple target server, where job is executed\n",
    "- create master server, in ssms go to sql server agent, right click, multi server administration, make this master\n",
    "- similar goes for create target server\n",
    "- connecting master server and target server need to consider  multiple factors like network/firewall/credential/etc. not part of the course \n",
    "\n",
    "### implementing policy using automated evaluation mode\n",
    "- policy are things we want to be true e.g. checking whether every db on vm has compatibility mode\n",
    "- only for vm\n",
    "- in vm, policy, create new policy, within which you can specify condition to check, then assign it against target (e.g. every db), then select the evaluation mode\n",
    "    - facet are things you need to check against\n",
    "    - evaluation mode:on demand/on schedule /pick existing schedule/ create new schedule\n",
    "    - change prevent will prevent change/ change log will log the change\n",
    "    - after evaluation, change could be directly apply on non-compatible target\n",
    "    - I want to create a policy to create a warning if my Recovery Model is \"Full\". then Recovery Model != full is needed. the condition should specify normal case\n",
    "\n",
    " \n",
    "- policy can be exported to hard drive or import to new vm later\n",
    "- policy can be turn into tsql code as well\n",
    "- in property policy against all facet can be checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30220a",
   "metadata": {},
   "source": [
    "# lesson 23 backup & restore for vm\n",
    "\n",
    "### automate backup\n",
    "- install sql server iaas agent extension can automate the backup for windows server 2012+ & sql server 2014+\n",
    "- in azure portal, vm, sql server config, manage sql vm, backup, enabled automate backup\n",
    "- retention period up to 30 days\n",
    "- we can select whether system dn (master model & mstb) should be backup\n",
    "- frequency is daily or weekly\n",
    "- starttime, lasting period, or automated backup schedule based on log growth where full recovery mode is needed\n",
    "- default instance/ single named instance can be backed up, but multiple named instance wo default instance will fail\n",
    "\n",
    "### db backup with options\n",
    "- mi is also possible, but mainly for vm. in mi copy only backup is checked and not much can be configured\n",
    "- in ssms right click on task, backup db, then select db, backup type(full/differential/transaction log)\n",
    "- in tsql\n",
    "    - MIRROR is only available with enterprise edition sql server, must be the same time as primary backup device, up to 3 secondaries are possible\n",
    "    - COPY_ONLY: usable for creating full backup\n",
    "    - DIFFERENTIAL: all the change since last full backup\n",
    "    - COMPRESSION: only in enterprise edition, overwrite server level default\n",
    "    - FILE_SNAPSHOT: create db file snapshot and store in blob\n",
    "    - NOINIT | INIT :whether the backup append or overwrite existing backup, noint will append the existing backup set\n",
    "    - NOSKIP | SKIP: checking expiration time\n",
    "    - NOFORMAT | FORMAT : whether media header be written on the volume used of backup operation, overwriting existing media header, format implies skip\n",
    "    - NO_CHECKSUM | CHECKSUM : validate backup by checking sum\n",
    "    - STATS = X : a percentage is displayed for every x %, default x=10  \n",
    "    - REWIND | NOREWIND : tape. tape is deprecated\n",
    "    - UNLOAD | NOUNLOAD : tape\n",
    "\n",
    "\n",
    "    ```\n",
    "    BACKUP DATABASE NameOfDatabase \n",
    "    [FILEGROUP = 'X', FILEGROUP = 'Y' …] \n",
    "    TO MyPreviouslyCreatedNamedBackupDevice = 'BackupDevice' \n",
    "    MIRROR TO AnotherBackupDevice \n",
    "\n",
    "    WITH -- Backup Set Options \n",
    "    COPY_ONLY \n",
    "    DIFFERENTIAL \n",
    "    COMPRESSION | NO_COMPRESSION \n",
    "    DESCRIPTION = 'description' \n",
    "    NAME = 'BackupSetName' \n",
    "    CREDENTIAL \n",
    "    ENCRYPTION \n",
    "    FILE_SNAPSHOT [EXPIREDATE = 'Dec 31, 2029 11:59 PM' | RETAINDAYS = days] -- Media Set Options \n",
    "    NOINIT | INIT \n",
    "    NOSKIP | SKIP \n",
    "    NOFORMAT | FORMAT -- Error Management Options \n",
    "    NO_CHECKSUM | CHECKSUM \n",
    "    STOP_ON_ERROR | CONTINUE_AFTER_ERROR -- Monitoring Options \n",
    "    STATS = X -- Tape Options \n",
    "    REWIND | NOREWIND \n",
    "    UNLOAD | NOUNLOAD \n",
    "    ; \n",
    "    ```\n",
    "- back up logs, must use full recovery model, \n",
    "    - NORECOVERY backup the tail of the log and leave db in restoring state, useful when failing over to a second db/ saving the tail of a log before restore operation/to restore additional backup\n",
    "    - NO_TRUNCATE: log is not truncated and require backup regardless of db status\n",
    "    - STANDBY: backup the tail of the log and leave db in read-only and standby state,  the file holds rolled back changes\n",
    "    ```\n",
    "    alter database dbname\n",
    "    set recovery full\n",
    "    go\n",
    "\n",
    "    BACKUP LOG NameOfDatabase   \n",
    "    TO MyPreviouslyCreatedNamedBackupDevice \n",
    "    WITH NORECOVERY, NO_TRUNCATE -- STANDBY = 'StandbyFileName' \n",
    "    ```\n",
    "### restore db\n",
    "- to restore a db, create db permission is needed (sys admin, db creator, fixed server\n",
    ", dbo owner) \n",
    "- in ssms, task, restore, db\n",
    "- a tail log db backup will be taken (last transactions)\n",
    "- lsn log sequence number, labeling to which log we are restoring\n",
    "- source is a list containing a db is backed up based on msdb history/ if there is a device (tape, url, file), the later is for when backup happen on a diff sql sever instance\n",
    "- select up to 64 device in 1 media set is possible\n",
    "- once a device is selected, a db on that device is available\n",
    "- data could be restore to a new db/ existing db (all db on server exclude master & temp, include model & msdb)\n",
    "- then select the time point to restore\n",
    "- mode:\n",
    "    1. restore with recovery: default, complete restore, full/bulk recovery mode, when restoring all logs at the same time\n",
    "    2. restore with no recovery: left in the restorng state, allows for additional backup\n",
    "    3. restore a standby: limited, readonly access\n",
    "- take a tail log backup allows future unwind of the restore\n",
    "- connection to server could be cloased, as running request might lead to failed backup\n",
    "- deleted db could also be restore through device, media set\n",
    "- in mi restore will happen in portal\n",
    "- using tsql for restoration \n",
    "    -\n",
    "    ```\n",
    "    RESTORE [dp300mi] FROM MyPreviouslyCreatedNamedBackupDevice  \n",
    "    WITH FILE = 6, NORECOVERY, STOPAT = 'Jun 19, 2024 12:00 PM'; \n",
    "    RESTORE [dp300mi] FROM MyPreviouslyCreatedNamedBackupDevice WITH FILE = 9, RECOVERY; \n",
    "    RESTORE VERIFYONLY FROM MyPreviouslyCreatedNamedBackupDevice \n",
    "\n",
    "    -- Restore from MI \n",
    "    RESTORE DATABASE NameOfDatabase \n",
    "    FROM URL = 'https:// … ' , 'https:// … ' \n",
    "    ```\n",
    "    - STOPAT for point in time restoration\n",
    "\n",
    "### backup and restore from and to cloud storage\n",
    "- create a storage account on azure portal\n",
    "- in ssms, db, task, backup, create to url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd5f5b",
   "metadata": {},
   "source": [
    "# lesson 24 hadr for h\n",
    "\n",
    "### transactional replication\n",
    "- change made in mi/vm can be pushed to a sql server db on premise/ azure vm or a instance db in azure sql mi\n",
    "- useful for distributing changes to more db/keeping several distributed db synchronised/migrating db by continuously publishing changes\n",
    "- publisher is azure sql mi/  sql  server instance , azure sql db only have data sync, can't be publisher\n",
    "- change made on some table is called article\n",
    "- update is the sent to distributor (sql mi/instance db), for sql instance it must be same or higher version than publisher\n",
    "- distributor can be the same as publisher just on different db\n",
    "- distributor then send info to subscribers \n",
    "    - pull subscriber (same type as distributor)\n",
    "    - push subscriber ( azure instance db)\n",
    "        - need not be 2 version earlier than publisher\n",
    "    - push subscriber ( azure sql mi)\n",
    "            - support transactional & snapshot & bi-directional mode\n",
    "    - push subscriber ( azure sql db)\n",
    "        - support only transactional & snapshot mode\n",
    "- in vm, ssms, replication right click local publication, new publication\n",
    "- publication type:\n",
    "    - snapshot distribute data for specific moment in time, doesn't monitor data update, subscriber can be db/mi/vm\n",
    "    - transactional change to occur in near real time, apply to subscriber in same order as in publisher, subscriber can be db/mi/vm\n",
    "    - peer to peer change in near real time. on multiple server instance, subscriber can be on-prem/vm\n",
    "    - merge data can be changed on both publisher and subscriber, subscriber can be on-prem/vm/mi\n",
    "- to create push subscription in vm, ssms, replication right click local subscription, new subscription\n",
    "\n",
    "### evaluate hadr\n",
    "1. mirroring \n",
    "    - one running in vm, the other on-perm for cross site disaster recovery\n",
    "    - server certificate for authentication\n",
    "    - no virtual network connection required, except when with same window server directory\n",
    "    - not support 2008, better at least 2012\n",
    "2. azure blob storage\n",
    "3. azure site recovery\n",
    "    - service to replicate workload on physical & vm, from primary to secondary location, if primary down, it can fail over to secondary\n",
    "4. log shipping\n",
    "    - one running in vm, the other onsite/cross site disaster recovery\n",
    "    - it ship log from primary to secondaries, like having many transactional backup but restore automatically\n",
    "    - depends on windows file sharing, so vpn is required\n",
    "    - replica domain controller installation is also recommended\n",
    "### availability group & windows server failover cluster\n",
    "- availability group requires a windows server failover cluster\n",
    "- a file share witness will check on all the replica and see if they are still up, then the replica and witness will vote, if majority vote the replica is not up, then it'll be deemed as down, then failover/notification to a secondary replica will be needed\n",
    "- 2-9 sql instance is possible, they can be on- perm data center or vm, with a domain controller, the data can be send from primary to secondary replica\n",
    "    - synchronous commit: the transaction from primary is not committed until it's committed on secondary for on-perm network\n",
    "    - asynchronous commit: suitable for geographically spread secondary, to avoid latency\n",
    "- hybrid mean on-prem and in cloud, if not hybrid then vpn is needed for failover cluster\n",
    "- for disaster recovery, replica domain controller at the disaster recovery site is also needed\n",
    "- availability group is also on azure\n",
    "1. create 3 vm on the same virtual network and resource group, one vm domain controller, one sqlserver1, one sqlserver2\n",
    "2. connect these 3 vm\n",
    "3. install windows failover cluster role on sqlserver1 & 2. in server manager, manage, add roles and feature, feature, failover clustering, install\n",
    "4. in azure portal, vm, sqlserver1 (then again 2), connect, rdp, remote desktop protocol  \n",
    "5. logging into sqlserver1 & 2, server manager, all server, right click server, failover manager, create cluster\n",
    "6. create a small db for ao and check connection from sqlserver1 to sqlserver2 is possible\n",
    "\n",
    "- always on availability group & failover cluster rely on underlying windows server failover clustering service (wsfc), it monitor network connection and health of nodes\n",
    "- quorum result help us to get odd number of vote on server status, which is a cloud witness\n",
    "- to configure quorum option:\n",
    "    1. in the portal create a storage account\n",
    "    2. click on storage account then access key, show key\n",
    "    3. on server, open failover manager controller, more action, configure cluster quorum setting\n",
    "        - default option: Computer decide what to do\n",
    "        - select the quorum witness\n",
    "        - advanced, configure cloud witness\n",
    "- witness type\n",
    "    1. cloud\n",
    "        - cloud witness use azure to provide a vote, ideal for deployment in multiple site, zone, and only a few megabyte\n",
    "        - cloud is recommended unless there is a failover group with shared storage\n",
    "        - cloud witness use general purpose and standard storage, blob/premium storage is not supported\n",
    "        - locally redundant storage for application time\n",
    "    2. disk\n",
    "        - this requires a small cluster disc in the cluster available storage group, the disc is highly available\n",
    "        - most resilient\n",
    "        - can failover multiple nodes/computer\n",
    "        - can only be used with cluster that has azure shared disc, cannot be a cluster shared volume\n",
    "    3. file share witness\n",
    "        - configured on a file server running windows server\n",
    "        - file share on a separate virtual machine in the same virtual network\n",
    "        - need to be separate from cluster workload to allow equal opportunity to other clusters\n",
    "        - least favorable\n",
    "    4. no witness, then it's just majority vote from the nodes\n",
    "- voting configuration: you could excludes other nodes (aka servers) from voting, and use only witness but not recommend\n",
    "- create availability group\n",
    "    1. on virtual machine, sql server configuration manager, sql server, property, enable, restart \n",
    "    2. in ssms, db, task, backup, do a full backup\n",
    "    3. in ssms, always on high availability, new availability wizard\n",
    "    - automatic failover require synchronized commit\n",
    "    - automatic failover, planned manual failover doesn't cause data loss\n",
    "    - forced manual failover & forced failover might lead to data loss\n",
    "    - asynchronized commit can only use forced manual failover \n",
    "    - min read write grace period for failover group is 1 hr\n",
    "    4. run the tsql below for granting necessary permission\n",
    "\n",
    "    ```\n",
    "    USE [master] \n",
    "    GO \n",
    "    CREATE LOGIN [NT AUTHORITY\\SYSTEM] FROM WINDOWS WITH DEFAULT_DATABASE=[master] \n",
    "    GO \n",
    "    GRANT ALTER ANY AVAILABILITY GROUP TO [NT AUTHORITY\\SYSTEM] \n",
    "    GO \n",
    "    GRANT CONNECT SQL TO [NT AUTHORITY\\SYSTEM] \n",
    "    GO \n",
    "    GRANT VIEW SERVER STATE TO [NT AUTHORITY\\SYSTEM] \n",
    "    GO\n",
    "    ```\n",
    "- right click on availability group can show dashboard\n",
    "- to trigger failover, click start failover wizard or tsql : alter availability group nameofgroup failover or powershell\n",
    "- with availability group listener, it allows to connect to whichever server that is available\n",
    "    - in ssms, availability group, availability group listener, add listener\n",
    "    - networkmode\n",
    "        - dhcp, dynamic host configuration protocol, computer pick particular network, not recommended\n",
    "        - static ip \n",
    "\n",
    "### log shipping\n",
    "- other disaster recovery option\n",
    "- continuously backup on-perm/sql db on vm every few mins, onto 1+ secondaries db each on diff sql server instance\n",
    "- limited readonly access to secondary db\n",
    "- need site to site vpn/express route for on-perm and vm connection\n",
    "- need to be in the same vnet/have vpn for vm to vm\n",
    "- not possible for azure sql db/mi \n",
    "- process\n",
    "    1. primary create transaction log backup that is stored somewhere\n",
    "    2. log backup is copied to secondary from storage\n",
    "    3. log backup restored on secondary db, db can't be read while restoring \n",
    "    4. optional monitor server to track any backup failure alert\n",
    "    5. when error happen, manual failover is needed\n",
    "- create log shipping:\n",
    "    1. create 3 vm on the same virtual network and resource group, one vm domain controller, one sqlserver1, one sqlserver2\n",
    "    2. connect these 3 vm\n",
    "    3. create new folder on server 1, share it with a user and copy the path, this is where primary store the log\n",
    "    4. create new folder on server 2, share it with the same user and copy the path\n",
    "    5. in ssms right click db, log shipping, property, transaction log shipping, enable log shipping, then in backup setting, paste in network path from 3 (the located server)\n",
    "    6. db state no recovery mode is recommended, unless looking into secondary is needed, standby mode and disconect user is recommended\n",
    "    - standyby not possible if secondary have later version than primary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90217e8",
   "metadata": {},
   "source": [
    "# practice test 1\n",
    "\n",
    "1. \n",
    "Data Migration Assistant (DMA) can help migrate and/or upgrade SQL Server to Azure SQL Database, or to a VM or to another on-prem server. It can also discover and assess SQL data estate, and recommend performance and reliability improvements for your target environment. It can detect compatibility issues between your current database and a target version of SQL Server or Azure SQL.\n",
    "\n",
    "SQL Server Migration Assistant (SSMA) is for migrating non-SQL objects, such as Access, DB2, MySQL, Oracle and SAP ASE databases to SQL Server or Azure SQL.\n",
    "\n",
    "Database Experimentation Assistant (DEA) is for comparing workloads between the source and target SQL Server. You can capture the workload of a source SQL Server environment and identity compatibility issues.\n",
    "\n",
    "Azure Database Migration Service (DMS) is for migrating open source databases, such as MySQL, PostgreSQL or MariaDB.\n",
    "\n",
    "2. If used Randomized encryption, then I wouldn't be able to use equality joins, GROUP BY, indexes and DISTINCT.\n",
    "\n",
    "4. Daily backups for SQL Server on an Azure VM use log and full backups - not differential backups.\n",
    "\n",
    "7. A nested loops join would probably be used in when the first table is small, and the second is large and has an index on the join. \n",
    "Merge joins are used when the inputs are not small, but are sorted on their join.\n",
    "Hash joins are used for large, unsorted, nonindexed inputs.\n",
    "A heap is a table without a clustered index.\n",
    "\n",
    "8. you can use backup compression on the Enterprise and Standard edition. As the Developer edition is the same as the Enterprise edition (but it cannot be used for commercial purposes), it can also be used in the Developer edition.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
