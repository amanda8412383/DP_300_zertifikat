{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1e27f3",
   "metadata": {},
   "source": [
    "# lesson 6 configure azure sql db for scale and performance\n",
    "\n",
    "**note:backup storage redundancy** \n",
    "\n",
    "geo redundant backup will replicate the storage to a paired region (e.g. US East to Us West) in case of data storage damaging\n",
    "\n",
    "**note:billing type** \n",
    "- provisional charged based on vcore\n",
    "- serverless charged based on second, shut down after 1 hr inactivity, restarted takes a little time\n",
    "\n",
    "\n",
    "**note:hybrid benefit** \n",
    "- it's a Microsoft licensing program that allows customers to use their existing on-premises Windows Server and SQL Server licenses with active Software Assurance or subscription to reduce costs when running those workloads in Azure\n",
    "- can only work with vcore, not with dtu\n",
    "**note:elastic pool** \n",
    "\n",
    "- let manage multiple db resource allocation possible, so they could share resource when one not using it\n",
    "- not possible for hyperscale\n",
    "- can select edtu (elastic dtu) in setting\n",
    "- unit price for edut compared to dtu is extra 50 %, vcore have same unti price\n",
    "- if a DTU db peak at different time, elastic pool could be a good idea, otherwise it's more expensive due to unit price\n",
    "- unavailable at hyperscale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### storage purchasing model\n",
    "- can be changed later, except changed out of hyperscale\n",
    "- change should be done at a low trafic time\n",
    "- after change DMV(dynamic management view) needed to be flushed for accurate estimation\n",
    "exec sq_query_store_flush\n",
    "##### DTU (database transaction unit)based\n",
    "- DTU can be adjusted with related S tier\n",
    "- CDC (changed data captured) can't be used when having less than 100 DTB (= S3 = 1vcore)\n",
    "- when using more than 300 DTU, vcore might be a cheaper option\n",
    "- plan selection can based on peak usage and storage need, the latter could be estimate with average data use * db number \n",
    "1. Basic: up to 2GB\n",
    "Development, testing, infrequently accessed workload\n",
    "2. Standard\n",
    "3. Premium\n",
    "suitable when more iops (inputs output per second) is needed\n",
    "##### vCore (virtual core) based\n",
    "- max data storgae and core could be specified\n",
    "- core decide max data storgae ceiling & tempdb size\n",
    "- log space is 30% of the max data stoarge\n",
    "- hardware choice affect memory, core number and storage\n",
    "- iops = concurrent number of request, this can't be configured, but go up with vcore\n",
    "\n",
    "4. general purpose: 80 vcore, 4 terabyte storage\n",
    "suitable for low latency input\n",
    "5. Hyperscale: 80 vcore, 100 terabyte storage\n",
    "6. Business critical: 80 vcore, 4 terabyte storage, fast geo-recovery and advanced data corruption protection, free second read-only replica\n",
    "Business critical has high transaction rate and high resiliency, is suitable for large number/long-running transaction\n",
    "\n",
    "### table partitioning\n",
    "\n",
    "- a table have 3 elements storage space, computing resource limit (exceeing might lead to timeout) and network bandwidth limit\n",
    "- vertical scaling by adding disc space, processing power, memory and network connection could be a short term solution\n",
    "- horizontal scaling by divifing the table up according to rule e.g. by year, it add up complexity but reduce hardware demand\n",
    "- back up per partition is also faster \n",
    "- the other option is to split the table columns up\n",
    "\n",
    "### database  sharding\n",
    "- when we apply horizontal scaling on db level\n",
    "- there would be a table with shard key, and where they are stored, this require more setting up \n",
    "- range strategy is diving by range e.g.years, sequential shard, data might be unblanced when there is seasonality\n",
    "- hashing strategy introduce random element for distribution, it reduce hotspot bcs no cluster would be more used, but rebalancing and manging mmight be difficult, when looking up a value the results are all over places\n",
    "- functional partitioning, not putting all the tables in the same db, e.g. seperate tables that needs extra security\n",
    "- keeping data geographically close can reduce latency when using\n",
    "\n",
    "**note: file group**\n",
    "- can't be used in Azure SQL db (only primary file group there)\n",
    "- file can be putted in file group to define partition range rules and where to store\n",
    "\n",
    "### compression for table\n",
    "- pro reduced space\n",
    "- con takes extra power and time to compress and retrieve \n",
    "- can't be used on table with sparse columns\n",
    "- changing compression require droping clustered index\n",
    "- complete index viewed can be compressed\n",
    "- system table can't be compressed\n",
    "- table with different partition can be compressed differently\n",
    "- *exec sp_estimate_data_compression_savings* help estimate compression effect\n",
    "\n",
    "method:\n",
    "1. no compression\n",
    "2. row compression: char and nchar columns would be greatly reduced, varchar and int not so much, date types depends, tiny int not at all\n",
    "3. page compression which includes (below element can't be activated seperately):\n",
    "    1. row compression\n",
    "    2. prefix compression: store long duplicated value in a specific column as prefix key, and generate an extra table for prefix key to long name to prevent the duplication of long char storage\n",
    "    3. dictionary compression: similar to prefix compression, but instead of on a specific column, the duplication would be searched across columns and keys would be used for whole table\n",
    "\n",
    "    - page is a set of row up to 8192 character\n",
    "    - uncompressed at first, when additional rows can't be fit in, compression takes place\n",
    "    - some older version SQL server on VM is this unavailable\n",
    "\n",
    "**note heap**\n",
    "haep is table without clustered index\n",
    "\n",
    "**columnstore table**\n",
    "above are row stored table. in columnstore table, columns are always compress, which could be further compressed through columnstore archival compression (but this has super high compute cost for uncompress)\n",
    "\n",
    "### SQL data sync\n",
    "- works for Azure SQL, on-prem or VM, but not with management instance\n",
    "- allows sync across db\n",
    "- primary key is required\n",
    "- must define a hub db and multiple member dbs\n",
    "- rules can be defined which direction to sync, e.g. change would be replicated from hub to member or member to hub, but not directly between members\n",
    "- rule when hub and member simultaneously changed should be defined\n",
    "- sync metadata db is an azure sql db whihc loacte in the same region as hub and store, should be empty at start\n",
    "- for on perm or vm member db, a sync agent programm is needed \n",
    "- on azure db page syncing db could be created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c84b1",
   "metadata": {},
   "source": [
    "# lesson 7 strategy of migration \n",
    "\n",
    "### migration tools\n",
    "\n",
    "| purpose    | tool | extra function |\n",
    "| -------- | ------- | ------- |\n",
    "| lift & shift sql server to vm   | azure migrate    |   assess sql data estate at scale(across data center)/getting azure sql deployment recommendation, target sizing & monthly estimate  |\n",
    "| non sql objects (DB2, MYSQL, Oracle, SAP, ASE) to sql/azure sql, each are independent download program | sql server migration system     ||\n",
    "| sql server object to sql db/managed instance/vm/on prem     | data migration system    |upgrade SQL/assess sql data estate at scale/ performance & reliability improvement recommendation/ detect incompatibility of target/ migrate on-prime/ discover new feature on target|\n",
    "| compare workloads between source and target  | database experiementation assistant    | |\n",
    "| open source db (mysql, postgresql, mariadb) to azure offline/online (premium price tier)  | azure db migration service    ||\n",
    "\n",
    "1. azure migrate  \n",
    "- consists of 2 tools: discovery & assessment, server migration\n",
    "**azure migrant hub** includes azure migration assistant & azure db migration service\n",
    "\n",
    "2. sql server migration system \n",
    "\n",
    "3. data migration system\n",
    "- for large size data azure db migration service  would be better option\n",
    "\n",
    "4. database experiementation assistant \n",
    "\n",
    "5. azure db migration service \n",
    "- princing tier standard provides up to 4 vcore for free, but only support offline migration\n",
    "- princing tier premium provides up to 4 vcore with 6 month free trial, support offline & online\n",
    "\n",
    "| from    | to | online (continuous sync)    | offline (one-time) |\n",
    "| -------- | ------- | -------- | ------- \n",
    "| SQL server  | azure sql db    | x  | v    |\n",
    "| SQL server    | azure sql db mi    | v | v    |\n",
    "| SQL server    | azure sql vm  | x  | v    |\n",
    "| mongodb    | azure cosmos db  | v | v    |\n",
    "| mySQL    | azure db for mySQL  | x  | v   |\n",
    "| postgreSQL    | azure db for postgresql  | v  | x  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**reminder** if not latest SQL is needed, then VM must be used and a window version needed to be decide\n",
    "\n",
    "**note** \n",
    "- without downtime allowance, the only way is to do an online migration\n",
    "- if cross db dependecy exist, azure db is not a good option, as it doesn't allow cross db queries\n",
    "- for azure db migration service, allow outbond port 443 = port for https or 1434 used for UDP, then enable TCP IP protocal, next create azure SQL db instance which needs a server level firewall rule on depature db/server & contorl server permission on depature server to allow access to DMS and on target db control db permission is needed. it use exsiting full log backup instead of creating new one\n",
    "- there are other service e.g. Bulk copie programm, command bulk insert, log data from Azure blob storage, SQL server integration service package, spark/azure factory\n",
    "- migrant stuff directly between Azure SQL db can use export or export data tier application (which allow export of schema), or export form azure portal, download and using SQLpackage\n",
    "- export data tier application (DAC) will make the export as .bacpac (backup package) file\n",
    "\n",
    "### post migration\n",
    "1. have tests to make sure data are there\n",
    "2. checking Query returns same result in sourece and target db\n",
    "3. checking the performance\n",
    "4. changing the app connecting to db from departure db to target db\n",
    "5. checking missin features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db218",
   "metadata": {},
   "source": [
    "# lesson 8 Microsoft Entra ID\n",
    "\n",
    "\n",
    "### authn = authentication = log in\n",
    "1. window authentication\n",
    "    - on a window machine/domain \n",
    "    - in azure only on VM is window available\n",
    "2. SQL sever authentication\n",
    "    - using username and password\n",
    "3. azure active directory\n",
    "    - setting could be access through azure portal, e.g. adding user\n",
    "    - more secure than SQL sever authentication and can select way to authenticate in portal \n",
    "    - once the AAD admin is setted through azure portal (only 1 admin allow), the admin can log in SSMS through (other log in method won't allow role creation) and create more roles through query \n",
    "    - comparison between AAD admin and sql server admin:\n",
    "        - both could create users based on SQL server authentication log in\n",
    "        - both can create contained DB users based on SQL server authentication wo log in\n",
    "        - AAD admin can create contained DB users based on aad users and group\n",
    "    - authentication methods includes FIDO secutity keys which is a hardware on computer, microsoft authenticator, text message and temporary access pass\n",
    "    - can sync with on-prem windows server active directory\n",
    "    - when we are logging in azure portal is a form of azure active directory\n",
    "    - azure active directory - integrated/universal with MFA(multi factor authentication) can be used when using admin tool like SSMS on a computer that is not domain joined \n",
    "    - 3 different way to authenticate in:\n",
    "        1. cloud only identity\n",
    "            - handle sign in completely in cloud by azure\n",
    "        2. federated authentication\n",
    "            - integrate with existing federation provider\n",
    "            - sign in requirement which is not native supported by Azure active directory\n",
    "        3. pass-through authentication\n",
    "            - when don't have a sign-in requirement not natively supported by Azure active directory\n",
    "            - and want to enforce extra feature like user level active directory sign in policy\n",
    "    - identity: \n",
    "        1. cloud only identity\n",
    "        2. hybrid identity support both cloud authentication & pass-through authentication\n",
    "        3. hybrid identity support federated authentication\n",
    "    - to create a user based on AAD user use: create user [user_email] from external provider\n",
    "4. window server active directory\n",
    "\n",
    "**note:**\n",
    "- authz = autherization = what one could access\n",
    "\n",
    "### role\n",
    "-  when running on an non-azure machine that is domain joined by creating a certificate and connect the app to azure data\n",
    "- to see all role in azure SQL use sp_helprole\n",
    "- after creating user, adding roles for the user is needed in SQL server\n",
    "    - alter role [role name] add/drop member [member email]\n",
    "    1. db_owner : have access to all.\n",
    "    2. db_securityadmin : modify role membership for custom roles only. manage permission, can elevate own permission\n",
    "    3. db_accessadmin: add & remove access to db for login & groups\n",
    "    4. db_backup_operator: not applicable in azure. can backup db, mi & vm\n",
    "    5. db_ddladmin: can run ddl comment (creat, alter, drop)\n",
    "    6. db_data_reader: can read all data\n",
    "    7. db_deny_data_reader: can't read old data\n",
    "    8. db_data_writer: ability to add, delete or change data\n",
    "    9. db_deny_data_writer: can't add, delete or change data\n",
    "    - on-prem only, roles that exist in master db\n",
    "    10. db_manager: create/delete db\n",
    "    11. login_manage: create/delete login in master db = security_admin for master db\n",
    "- role based access control (RBAC) in azure:\n",
    "    - Access control section is called IAM, when role could be added\n",
    "    - there is no access to server wide logging permission bc access to underlying server doesn't exist\n",
    "    1. SQL DB contributer: allow SQL DB, but don't have access to content\n",
    "    2. SQL server contributer: allow SQL server/security policy manage, but don't have access to content\n",
    "    3. SQL security manager: allow SQL security policy manage, but don't have access to content\n",
    "\n",
    "### granting permission\n",
    "- granting access to particular schema/table can't be achieved through these set roles in stead grant permission command should be used\n",
    "- command: authorization permission on securable::name to pricipal with grant option\n",
    "- select * from sys.fn_my_permissions(null, 'DATABASE') shows all permission\n",
    "1. autherization (grant, revoke, deny)\n",
    "    - grant [permission_type] on object::[schema].[table] to [user_email]\n",
    "    - to revoke the permission used revoke [permission_type] on object::[schema].[table] to [user_email]\n",
    "    - to ban someone from table access that cannot be overwrite by grant use deny [permission_type] on object::[schema].[table] to [user_email]\n",
    "2. permission \n",
    "    - table: select, insert, update, delete, control(all right to table), reference(view foreignkey), take ownership(change ownership), view change tracking, view definition (rightclick and go to script table path)\n",
    "    - schema: alter(=alter/create/drop any securable)\n",
    "    - function/stored procedure: alter, execute(even lack of select permission on table is not a problem, this is called ownership changing), view change tracking, view definition\n",
    "3. securable (object, schema, db, server)\n",
    "4. principal (login, user, role: create role [rolename], add member [useremail])\n",
    "    - public is a special role that every single user of the db have this role\n",
    "5. with grant option: allow the principal to share the permission later\n",
    "\n",
    "**note:apply principal of least privilige for all securable**\n",
    "a user should have least privilige user account (LUA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1cbe2",
   "metadata": {},
   "source": [
    "# lesson 9 implement security\n",
    "\n",
    "### transparent data encryption (tde)\n",
    "- this is the security for data in transit\n",
    "- it encrypts then deencrypts data at page level at rest\n",
    "- encrypts at write then decrypts at read\n",
    "- for azure SQL db this is set at server level, new db inheritate the rule unless recover from a copy, encryption can be turn off in azure portal / Tsql / cloudshell / restapi\n",
    "- for azure sql mi this is set at instance level,  new db inheritate the rule but not system db, encryption can be turn off  in Tsql/ cloudshell/ restapi\n",
    "- it use database encryption key (dek), which is a symetric key = one single key is needed for en- & decrypts\n",
    "    - this could be a service managed key, protected by a TDE protector using service managed certificate \n",
    "    - this could be a bring your own key, in this case it could also be asymmetric\n",
    "\n",
    "**note** transparent layer security (TLS), encrypts when in transit\n",
    "\n",
    "### object level encryption (e.g. table with sensitive data)\n",
    "1. always encrypted\n",
    "    - always encrypted is available in azure sql db, mi, vm\n",
    "    - this could be done in ssms through right click encrypt columns\n",
    "    - encryption type has deterministic (encryption is the same everytime, allows for join, group by, index, distinct without decrypt) & random \n",
    "    - when the column is used as index, it can't be randomized encrypt\n",
    "    - to create encryption, key vault is needed\n",
    "    - after creation, next time when connecting to db in SSMS select enable alway encrypt \n",
    "    - a pricipal (e.g. user) need to be in key vault to decrypt encrypted colunm\n",
    "\n",
    "    - to query an encrypted column in where clause, set up a parameter & enable parameterlization for always encrypted is needed, but this will still block funtion like\n",
    "    - to enable partial string comparison, always encrypted need to be used with secure enclaves\n",
    "        1. enable confidential computing through configured as DC series\n",
    "            - strongest security isolation\n",
    "            - type name is intel software guard intellegence(sgx)\n",
    "            - require ms azure attestation\n",
    "        2. enable secure enclaves in a standard configure db\n",
    "            - type name is virtualization based security (vbs)\n",
    "            - available for all ay sql, and sql 2019+\n",
    "            - not available in ger india central\n",
    "            - can't defend against e.g. replacing enclaves program with malware\n",
    "            - creating a protected part of memory within which data is display as plain text\n",
    "            - protection against operative system threat\n",
    "            - standard azure protection (e.g. multifactor authentication, just in time access)\n",
    "            - can't be deactivated after activation\n",
    "\n",
    "    - permission wise there are\n",
    "        1. alter any column master key\n",
    "        2. alter any column encryption key\n",
    "        3. view any column master/encryption key (for access, query and read encrypted data)\n",
    "    - role wise there are\n",
    "        1. security admin who generate column encryption key and master key, access to key and key store are required, access to db is not required\n",
    "        2. db admin who manage metadata about key, access to key and key store isn't required, access to db is required\n",
    "        - if these 2 role should belongs to 2 ppl, then powershell should be used\n",
    "        - if these 2 role should belongs to same ppl, then powershell or ssms could be used\n",
    "\n",
    "2. dynamic data masking\n",
    "    - this could be done through azure portal \n",
    "    - in tsql: alter table [schema].[table] alter column [column] add masked with (fuction = 'default()')\n",
    "    - it can be apply on specific column with desired partial masking 2025-0x-1x\n",
    "    - specific users could be excluded in azure portal/tsql: grant(/revoke) unmask to [useremail] \n",
    "    - this could also be done through powershell\n",
    "    - mask type\n",
    "        - default value 0\n",
    "        - number: choose random number between 2 different bundary\n",
    "        - email: expose first value of email\n",
    "        - credit card: last 4 digits of card number\n",
    "### key vault \n",
    "- could be created in azure sql portal\n",
    "- in key vault store secrets\n",
    "- key vault name need to be unique\n",
    "- better store it in region near db\n",
    "- pricing tier premium allow use of hardware security model (HSM). it's the same as standard  until hsm is used\n",
    "- in access policy selecting unwrap key + wrap key + verify + sign (and also the default selections) is necessary to column create primary key \n",
    "\n",
    "\n",
    "| | always encrpted    | transparent data encrpytion (tde) |\n",
    "| -------- | ------- |------- |\n",
    "| sql server version  |  2016+   | 2008+ |\n",
    "| sql server entriprise edition | x     | v|\n",
    "| free in azure sql db    | v    | v|\n",
    "| protect data at rest   | v   | v |\n",
    "| protect data in use    | v  | x (when in transport, using transport layer security) |\n",
    "| protect data from sql admin  | v  | x |\n",
    "| data is de/encrypted on the client side  | v | x|\n",
    "| data is de/encrypted on the server side   | x | v |\n",
    "| encrypt at    | column level  | entire db|\n",
    "| transparent to application   | partially | v |\n",
    "| encryption option   | v | x |\n",
    "| key management   | customer managed keys | server/customer managed keys |\n",
    "| protects key in use  | v | x |\n",
    "| driver required | v |  x|\n",
    "\n",
    "\n",
    "### configure server/db level firewall\n",
    "- by default all connection are rejected, unless firewall is set up\n",
    "- db rule would be checked before server rule when implement\n",
    "- server rule need to be set before db rule\n",
    "- doesn't apply to azure sql mi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|| server   | db |\n",
    "| ------- |------- |------- |\n",
    "|| for user and app to have access to all dbs in the server| for an individual/app |\n",
    "|can be set in azure portal|v | x|\n",
    "|role|sql server contributor/sql security manager/owner of the resource | control db permission is required at db level|\n",
    "|tsql to check rules|select * from sys.firewall_rules  | select * from sys.database_firewall_rules |\n",
    "|tsql to set rules|execute sp_set_firewall_rule @name=N'rule', @start_ip_address='ip', @end_ip_address='ip' (need to be in masterdb)| execute sp_set_database_firewall_rule @name=N'rule', @start_ip_address='ip', @end_ip_address='ip' |\n",
    "|tsql to delete rules|execute sp_delete_firewall_rule @name=N'rule' (need to be in masterdb)| execute sp_delete_database_firewall_rule @name=N'rule'|\n",
    "\n",
    "**note** n before quote change varchar to nchar/nvarchar\n",
    "\n",
    "### transport layer security (tls)\n",
    "- encrypt data on db/client side and decrypt it from other side\n",
    "- most widely used is TLS 1,2, available in 2008 and is used in azure sql db\n",
    "- if the tls version from cleint older than min version allowed in target db , connection would fail\n",
    "- min tls version can be set in powershell/azureportal\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2245f",
   "metadata": {},
   "source": [
    "# lesson 10 & 11 implement compliance control for sensitive data\n",
    "\n",
    "### data classification\n",
    "- sensitive data include data privacy, regulatory and national requirement\n",
    "- classification can be set in azure portal, it will also show recommanded classification \n",
    "- information type and security label can't both be n/a\n",
    "- security label:\n",
    "    - n/a : available to all \n",
    "    - public: public released data\n",
    "    - general: data not mean for public e.g. emails, docs\n",
    "    - confidential\n",
    "    - confidential - GDPR\n",
    "    - highly confidential\n",
    "    - highly confidential - GDPR\n",
    "- role to change classification: db_owner, db_contributor, sql_security_manager\n",
    "- role to read classification: reader, user access admin\n",
    "- tsql to check: select * from sys.sensitivity_classifications\n",
    "- tsql: select * from sys.columns where object_id = [majorid from above query]\n",
    "- tsql to add: add sensitivity classification to [schema].[table].[column name] (with label='label name', information_type='info name', rank=low/medium/high/critical/none)\n",
    "- tsql to drop: drop sensitivity classification to [schema].[table].[column name] \n",
    "\n",
    "### configure server and db audits\n",
    "- audit retain trails of selected db actions, report db activity using preconfigured report on dashboard and analyse report for suspicious event\n",
    "- not supported for premium storage/hierarchical namespace\n",
    "- having server and db audit at the same time is possible, but only using server level is recommand, unless specific requirement for specific db is needed\n",
    "- default policy includes batch completed group, all the queries & storage procedure, successful db & failed db authentication group (e.g. login)\n",
    "- it store around 4000 character in an audit \n",
    "- server policy audit always apply to db disregarding db level audit policy\n",
    "- audit log storage:\n",
    "    1. existing/ new storage account\n",
    "    2. existing monitor log analytics workspace\n",
    "    3. existing event hub\n",
    "- audit log can be viewed in azure portal or ssms merge audit file and add from azure blob storage\n",
    "\n",
    "### data change tracking\n",
    "- both can be used in azure sql db, but only change data capture can be usedd in aure sql mi\n",
    "1. change tracking (ct)\n",
    "    - when a row/column get changed it will be tracked\n",
    "    - doesn't track how many time it has changed/history data\n",
    "    - it store less data than adc\n",
    "    - it store data in an in-memory roaster and flushed on every checkpoint to the internal data (it kept in memory and then saved often)\n",
    "    - snapshot isolation\n",
    "        - SQL Server recommends using snapshot isolation with change tracking\n",
    "        - Snapshot isolation ensures that the change tracking process itself operates on a consistent snapshot of the data, preventing issues where changes might be missed or incorrectly reported due to concurrent modifications \n",
    "        - using snapshot isolation will makes any changes while geting data invisible (it has a set of data instead of a changing set)\n",
    "        - tsql for snapshot isolation:ALTER DATABASE [dbname] SET ALLOW_SNAPSHOT_ISOLATION ON then SET TRANSACTION ISOLATION LEVEL SNAPSHOT\n",
    "    - enable ct can be done in ssms /tsql, it has to be done first on db level then for specific tables\n",
    "    - for tracking updates command, extra parameter need to be set\n",
    "    - tsql for enabling ct: alter database mydb set change_tracking = on (change_retention = 2 days, auto_cleanup = on)\n",
    "    - to disable, all tables need to be disabled then the db\n",
    "    - tsql for Check which tables/databases have CT enabled: \n",
    "\n",
    "        SELECT * from sys.change_tracking_databases \n",
    "        select * from sys.databases \n",
    "        SELECT * from sys.change_tracking_tables  \n",
    "        select * from sys.objects \n",
    "    - tsql for getting the initial sync version:\n",
    "\n",
    "        DECLARE @last_sync bigint; \n",
    "        SET @last_sync = CHANGE_TRACKING_CURRENT_VERSION();  \n",
    "        select @last_sync \n",
    "        select CHANGE_TRACKING_CURRENT_VERSION() \n",
    "\n",
    "    - tsql for finding out what after changes have happened: \n",
    "\n",
    "        SELECT  CT.AddressID, CT.SYS_CHANGE_OPERATION,   \n",
    "        CT.SYS_CHANGE_COLUMNS, CT.SYS_CHANGE_CONTEXT \n",
    "        FROM  CHANGETABLE(CHANGES SalesLT.Address, 0) AS CT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. change data capture(cdc)\n",
    "- not available in basic price tier, available in standard price tier from s3\n",
    "- need to first enable for db then for table\n",
    "- tsql for Enable for database (can't be used in system db) :\n",
    "    EXEC sys.sp_cdc_enable_db  \n",
    "- tsql for Enable for table :\n",
    "\n",
    "    EXEC sys.sp_cdc_enable_table @source_schema = 'SalesLT', @source_name = 'Address', \n",
    "    @role_name = 'NewRole', \n",
    "    @captured_column_list = 'AddressID, City'\n",
    "\n",
    "- tsql for checking configuration:\n",
    "\n",
    "    EXECUTE sys.sp_cdc_help_change_data_capture \n",
    "\n",
    "- tsql for Disable from table :\n",
    "\n",
    "    EXEC sys.sp_cdc_disable_table @source_schema = 'SalesLT', @source_name = 'Address', \n",
    "    @capture_instance = 'all' \n",
    "\n",
    "- tsql for Disable for database :\n",
    "\n",
    "    EXEC sys.sp_cdc_disable_db \n",
    "\n",
    "- tsql for checking changes:\n",
    "\n",
    "    DECLARE @from_lsn binary(10), @to_lsn binary(10);   \n",
    "    SET @from_lsn = sys.fn_cdc_get_min_lsn('SalesLT_Address');   \n",
    "    SET @to_lsn = sys.fn_cdc_get_max_lsn();   \n",
    "    SELECT * FROM cdc.fn_cdc_get_all_changes_SalesLT_Address  (@from_lsn, @to_lsn, N'all')\n",
    "\n",
    "    **note** lsn = log sql number\n",
    "\n",
    "    **note** cdc.fn_cdc_get_all_changes_SalesLT_Address is a custome sql function made by cdc, it has name of the specific table, argument are from, to , and display type\n",
    "\n",
    "### vulnerability assessment\n",
    "- a defender for Servers plan is required to do assessment, azure defender cost 15 dollar per month per server\n",
    "- adding a failed test as baseline will change the failing status\n",
    "\n",
    "### azure purview\n",
    "- when data is stored in different places it's hard to check for compliance\n",
    "- purview catalogues the data regardless of on premises, in a machine, online, on cloud using ssas\n",
    "- accessing new data source from purview\n",
    "    1. azure service need to allow connection from azure service in server setting for purview to work\n",
    "    2. a secret (e.g. sql password) in keyvault (using purview resource group) need to be added\n",
    "    3. in access policy from key vault give permission to purview, with key permission get & list\n",
    "    4. in purview credential, add the secret from step 2\n",
    "- using purview\n",
    "    1. create ms purview account (need subscription & resource group )\n",
    "    2. open ms purview portal which shows 3 main element:\n",
    "        1. data map: matadata created through classifying and scanning varios sources\n",
    "        2. data catalogue: help with finding data with classification/metadata filter\n",
    "        3. data insight: locate sensitive data\n",
    "    3. using scan rule set which help group together classification & file type\n",
    "- collection & scan \n",
    "    - collection within collection is allowed\n",
    "    1. add a source & selecting which source it should be linked to\n",
    "    2. click scan on the added source, define scan table, targeted data type & time schedule\n",
    "    - lineage extraction is limited to store procedure run, otherwise should be turned off\n",
    "\n",
    "\n",
    "### azure db ledger\n",
    "\n",
    "- ledger provides a history of changes made to a database's data, ensuring data integrity and enabling auditing\n",
    "\n",
    "**what is updatable ledger table**\n",
    "- updatable ledger table provide cryptogrphic (a security communication technique) proof of date to the auditors. it reduce time to audit data. \n",
    "- all the modification that is done to updatable ledger table is hashed cryptographically using SHA256 (= a root hash). \n",
    "- a hash is similar to a footprint that is left behind by the footstep. root hash are stored in blocks. blocks are closed after 30 sec/ 100,000 transactions, then hashed along with a root hash pf Ã¼revious block, forming a blockchain0.\n",
    "- the latest hash block is called database digest\n",
    "**how updatable ledger table is stored**\n",
    "- db ledgers are stored in trusted storage e.g. immutable azure blob storage(which follows write one read many times = can't be altered), azure credential ledger\n",
    "- data integrity can be verified by comparing db digest hash against calculation of current table\n",
    "- sql db manage db ledger transparently\n",
    "**how updatable ledger table created**\n",
    "- non ledger table can't be converted into ledger table\n",
    "- data need too be copied into new ledger table(select into/bulk insert/sp_copy_data_in_batch, sp=stored procedure), then rename ledger table as original table name\n",
    "    - benefit of sp_copy_data_in_batch is it split copy operation into batches, 10 - 100 k per transaction done in parallel, therefore speed up the process\n",
    "1. in ssms go to table, right click, script table as\n",
    "2. copy the script to create a new table, with new name and adding tsql\n",
    "    for updatable: with (system_versioning = on, ledger = on)\n",
    "    for append only: with (ledger = on,append only = on)\n",
    "3. insert everything from original table into this ledger table\n",
    "4. now the history table can be seen in sys.table if updatable is used\n",
    "5. could query from the ledger view\n",
    "\n",
    "**what's in updatable ledger table**\n",
    "- ledger table have 4 new cols, named as always generated columns, automaticallly generated \n",
    "    1. ledger_start_transaction_id: unique transaction id of each inserted action, e.g. if 10 rows are inserted at the same time, they'll all have the same id\n",
    "    2. ledger_end_transaction_id\n",
    "    3. ledger_start_sequence_id: e.g. if 10 rows are inserted at the same time, they'll have sequence from 0 to 9. for next transaction sequence starts from 0 again\n",
    "    4. ledger_end_sequence_id\n",
    "\n",
    "**what is history table**\n",
    "- it's also built on db ledgers\n",
    "- it shows previous version of a row, when it's updated/deleted in the updatable ledger table\n",
    "- it has also 4 always generated columns\n",
    "- data can't be deleted from here\n",
    "- automatically named as original table name.mssql_ledgerhistoryfor_guid\n",
    "\n",
    "**what is ledger  view**\n",
    "- it join updatable ledger table and history table\n",
    "- it shows transaction id, delete/insert/update(update is both delete & insert)\n",
    "- is recommanded to used instead of the other 2 tables\n",
    "\n",
    "**append-only ledger table**\n",
    "- another version of ledger table, - it's also built on db ledgers\n",
    ", alternative to the updatable ledger table\n",
    "- data can be insert but not update/delete\n",
    "- in this case history table is not needed\n",
    "- there is still a ledger view, provide info on transaction & user who inserted data\n",
    "- 2 always generated columns\n",
    "    1. ledger_start_transaction_id: \n",
    "    2. ledger_start_sequence_id: \n",
    "\n",
    "**verifying**\n",
    "1. in azure portal, select digest storage, adding a storage container\n",
    "2. go to verify db option, which provides code to look at data_ledger_digest_location \n",
    "    - storage container could also be activated in security tab when creating azure sql db\n",
    "    - when creating azure sql db if select ledger db, then all the table in this db will by default all and only be ledger tables\n",
    "3. copy this and run in ssms, it'll show:\n",
    "    - digest location (path of the digest)\n",
    "    - last verified blog id \n",
    "    - whether the path is the latest location, if unsuccessful then db has been tempered with\n",
    "4. if it's been tampered with restored to a verifiable version is needed, then manually create any future transaction while using backups after the point of time\n",
    "\n",
    "\n",
    "### row level security\n",
    "- available for sql 2016 +\n",
    "- actions like select, update and delete could be restricted to specific rows\n",
    "- block write operations on new/existing data: after insert/after update\n",
    "- block update/delete on existing data: before delete/before update\n",
    "1.  it's recommanded to have a seperate schema for row level security object, it helps reducing effort in maintaining various permission \n",
    "2. create inline table valued function (tvf), it returns when user should see the result: \n",
    "\n",
    "    create function rls.rls_security(@user an nvarchar(10)) returns table with schema binding \n",
    "\n",
    "    as return select 1 as result \n",
    "\n",
    "    where @user = username()\n",
    "3. add select permission to the tvf: \n",
    "\n",
    "    grant select on rls.rls_security to [user1] \n",
    "\n",
    "    grant select on sceham.table to [user1]\n",
    "4. create security policy (role needed): \n",
    "\n",
    "    create security policy policyname \n",
    "    \n",
    "    add filter predicate rls.rls_security(usercolname )\n",
    "    on sceham.table\n",
    "\n",
    "    **note** this filter out what shouldn't be seen\n",
    "\n",
    "    add block predicate rls.rls_security(usercolname )\n",
    "    on sceham.table after insert\n",
    "\n",
    "    **note**  block insert outside of what allowed to be seen\n",
    "\n",
    "    with (state = on)\n",
    "\n",
    "    go\n",
    "- without turining the security policy on, everyone will be able to view all rows, after turning on, only the user being granted access can\n",
    "\n",
    "### configure advanced threat protection\n",
    "- available for azure sql db, mi, vm\n",
    "- it allow admin to be alerted when potential threat happened e.g. suspicious pattern on querying/access, possible vunerability, sql injection attack\n",
    "1. enable ms defender for cloud\n",
    "2. click on the configure and set up email notification for advanced threat protection alert\n",
    "3. the email will also show the suggested action for fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49534a6",
   "metadata": {},
   "source": [
    "# lesson 12 monitoring activity and performance\n",
    "\n",
    "### creating performance base line\n",
    "- performance are affected by hardware (pricing tier)\n",
    "- on vm performance is also affected by operation system, db applications and client application\n",
    "- with a high dtu percentage(= processor percentage, Database Transaction Unit, it represents a blended measure of CPU, memory, reads, and writes) or i/o percentage :\n",
    "    1. more cpu/input output resource is needed \n",
    "    2. query needs to be optimized\n",
    "1. metrics explorer\n",
    "\n",
    "    - metrics are collected at regular interval and stored in a time series db\n",
    "    - they have a timestamp for when, name, value and other label\n",
    "    - it's light weighted and allows for near real time allerting\n",
    "    - in monitoring section, metric tab allows one to see metric for a specific sql db\n",
    "    - max 30 days at once \n",
    "    - up to 93 days in the past is viewable\n",
    "\n",
    "2. log\n",
    "    - operation performance metrics can also be created by using log\n",
    "    - logs can be structured or free form, may have a timestamp\n",
    "    - in monitoring> logs> diagnostic> loading data, last hour logs can be imported using language kusto\n",
    "\n",
    "3. tsql\n",
    "    - SELECT * from sys.dm_db_resource_stats  \n",
    "        - current db or elastic pool CPU, IO and memory \n",
    "        - a row every 15 s, for last 1 hr\n",
    "    - SELECT * from sys.server_resource_stats  \n",
    "        - equivalents for reports on azure sql mi, CPU, IO and memory \n",
    "        - a row every 15 s, for last 1 hr\n",
    "\n",
    "    - SELECT * from sys.resource_stats \n",
    "        - reports on server( multiple db), CPU, IO and memory \n",
    "        - a row every 15 s, for last 1 hr\n",
    "\n",
    "    - SELECT * from sys.dm_user_db_resource_governance  \n",
    "        - Returns one row actual configuration and capacity settings used by resource governance mechanisms in the current database or elastic pool\n",
    "        \n",
    "    - SELECT * from sys.resource_usage  \n",
    "        - provides hourly summary of result usage data for user db in current server\n",
    "        - even if db is idle, as long as it's accessible, it'll have 1 row/hour \n",
    "        - the view doesn't contain usage anymore, only storage in megabyte\n",
    "    - exec sp_who \n",
    "        - show current user , process, whether it's blocked\n",
    "    - exec sp_lock \n",
    "        - show lock, object id, index id\n",
    "        - Locks are held on SQL Server resources, such as rows read or modified during a transaction, to prevent concurrent use of resources by different transactions   \n",
    "    - exec sp_spaceuser\n",
    "        - display estimation of current disc space use     by a table or db \n",
    "    - exec sp_monitor\n",
    "        - work for on prem db but not for azure sql db\n",
    "        - show cpu usage, io usage, idle time etc\n",
    "\n",
    "**note** An on-premises database, refers to a database system that is hosted within a company's own facilities, typically in their own data center or server room\n",
    "\n",
    "### create event notifcation\n",
    "- alert could be created from alert, metric or log tab in azure portal\n",
    "- how frequent the measure being group together and how frequent the alert rule being checked need to be selected when creating the rule, if grouped every 30 min, checking frequency can't be lower than every 30 min\n",
    "- static threshold would be based off of a fixed figure\n",
    "- dynamic threshold would be automatically decided by machine, taking into account factors like seasonality\n",
    "    - in dynamic mode how sensitive to the standard and how many number of violation within a arbitrary time period to trigger the alert, notice this should be a reachable goal with the checking frequency, default value is medium sensitive\n",
    "- then action and notification type could be selected for the alert\n",
    "- automatically resolve alert mean when value gets back to normal, resolve the alert automatically\n",
    "\n",
    "\n",
    "### source for performing metrics\n",
    "1. Azure tenant\n",
    "    - tenant wide service: azure active directory, \n",
    "2. azure activity log: include self service health record, configuration changes\n",
    "3. azure service health\n",
    "4. azure resources\n",
    "    - submit platform metrics to metrics db\n",
    "    - reource log then create internally regarding internal operation of resources\n",
    "    - for guest operating system, azure dignositc extension for vm if enabled or log analytic agent that can install on machine or vm insight\n",
    "\n",
    "\n",
    "### interpreting performace metrics\n",
    "- system dmdb resource stats: if any of that is close to 100 %, then upgrade the service plan might be needed, if too low downgrade can save money\n",
    "- xtp storage percent: percentage of storage used by In-Memory OLTP objects in Azure SQL Databas\n",
    "    - 0 if in memeory optimized table not used\n",
    "    - 100 update and insert will fail, select and delete are fine\n",
    "- max session percent: the max concurrent session allowed/service tier limit\n",
    "- max worker percent: max concurrent request/service tier limit\n",
    "- in intelleigence performance section in azure portal, in query performance insight and see the most heavy query by cpu/data io/ log io, some of them come with performance recommendation\n",
    "\n",
    "### intelligent insight\n",
    "\n",
    "- it compares current db workload to last 7 days performance\n",
    "- looks for things that could affect db performance e.g.resourcing limit, worklaod increase, memory pressure (workers waiting for memory allocation), data locking or need to increase maximum degree of parallelism = dop, missing index, new query affected performance, multiple thread using same temp db resource\n",
    "- automatically enable for azure sql db, need to manual enable for azure mi\n",
    "- ai detect high wait time/query prioritization/\n",
    "- not available for vm and some europe/us region\n",
    "- include \n",
    "    - query performance insight \n",
    "    - diagonostic setting\n",
    "        - log type includes\n",
    "            - SQLinsights\n",
    "            - automatic tuning\n",
    "            - timeouts\n",
    "        - destination, where the log can be sent to includes\n",
    "            - log analytics workspace\n",
    "            - storage account\n",
    "            - stream to event hub\n",
    "            - partner solution\n",
    "\n",
    "\n",
    "| event/ activity  | extended events | sql server profiler |distributed replay |system monitor = performance monitor | activity monitor in ssms| transact sql| error log|performance dashboard in ssms|\n",
    "| :----- | :------ | :------: |:-------: |:------: |:------: |:------: |:------: |-----: |\n",
    "| what it is  |   light weight monitoring system, using littele resource, allow create, modify, display, analyse session data   | track process event, monitor service and db activity |sql server profiler to replay on multiple machine, simulate a mission critical workload | track resource usage, monitor server performance using counters and rates| display info of process running on sql instance|   | window app event log (for vm) | identifying performance buttleneck in sql server|\n",
    "| trend analysis           | v | v |   | v |   |   |   |   |\n",
    "| replaying captured event |   | v(data can be save as sql table, replay step by step from a single computer) | v(data can be save as sql table, replay step by step from a multiple computer)a| | | | | |\n",
    "| adhoc monitoring         | v (xevent profiler)| v |   |   | v | v | v | v |\n",
    "| generating alarm         |   |   |   | v |   |   |   |   |\n",
    "| graphical interface      | v | v |   | v | v |   | v | v |\n",
    "| using within custom app  | v | v |   |   |   | v |   |   |\n",
    "\n",
    "### sql insight \n",
    "- retired at 2024 but still part of the exam\n",
    "- ms allows 1/more dedicated azure virtual machine (ubuntu 18.04, size b2s) to collect info from azure sql db, mi, vm then store it in log analytics space\n",
    "- sql insight/workbook template/log queries connects to this space, it's part of azure monitor\n",
    "- below s1, elastic pool, some price tier plans are not supported, bcs it constantly querying the db, its not suitable for serverless\n",
    "- require sql server authentication\n",
    "\n",
    "### db watcher\n",
    "- a centralized store for performance, configuration, health data for azure sql db & mi & elastic pool (* are also available for elastic pool)\n",
    "    - active history\n",
    "    - backup history\n",
    "    - change processing and error\n",
    "    - connectivity\n",
    "    - (mi: db)geo replica\n",
    "    - index metadata\n",
    "    - memory utilization *  \n",
    "    - missing index\n",
    "    - out of memory event *\n",
    "    - performance counter *\n",
    "    - db and instance property *\n",
    "    - query runtime and wait statistic\n",
    "    - db replica *\n",
    "    - resource utilization\n",
    "    - session statistic\n",
    "    - sos schedueler *\n",
    "    - sql agent job can only be stored on mi\n",
    "    - storage io *\n",
    "    - storage utilization *\n",
    "    - table metadata\n",
    "- it gets data from db and store it in :\n",
    "    - azure data explorer cluster: highly scalable data service for fast input and analytic \n",
    "    - microsoft fabric: realtime analytic \n",
    "- visualization is free, storage is not\n",
    "- once store can be query using kusto kql or tsql in azure data explorer dashboards, powerbi, gafana, excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef69cf4",
   "metadata": {},
   "source": [
    "# lesson 13: manage performance related maintainence task\n",
    "### index maintainence task\n",
    "\n",
    "- select * from sys.dm_db_missing_index_details \n",
    "\n",
    "- select * from sys.dm_db_physical_stats \n",
    "    -access fragmentation of db index\n",
    "- select * from sys.dm_db_column_store_row_group_physical_stats \n",
    "    -access columnstore index\n",
    "    - deleted row / total row = how fragmented index is\n",
    "    - when a row in column stored get updated, it is marked as delete\n",
    "\n",
    "-   ALTER INDEX ALL \n",
    "    ON [SalesLT].[Address] \n",
    "    REORGANIZE \n",
    "    - Reorganize indexes for fragmentation between 10-30 %, columstore for above 20&\n",
    "\n",
    "-   ALTER INDEX [PK_Customer_CustomerID] \n",
    "    ON [SalesLT].[Customer] \n",
    "    REBUILD WITH (ONLINE = ON,  \n",
    "    FILLFACTOR = 60, \n",
    "    MAX_DURATION = 30, \n",
    "    RESUMABLE = ON) \n",
    "\n",
    "    - Rebuild indexes  for fragmentation above 30 %\n",
    "    - fullfactor allow create 135 on the page (60% filled), and leave 2 and 4 for later, the index will be bigger due to blank page\n",
    "    - max_duration = time limit \n",
    "\n",
    " -  ALTER INDEX [PK_Customer_CustomerID] \n",
    "    ON [SalesLT].[Customer] \n",
    "    PAUSE/ABORT /RESUME \n",
    "    - pause/abort/resume an alter\n",
    "\n",
    "### statistic maintainence task\n",
    "- use to create query plan to improve the speed, estimate cardinality(# of rows), allow seek instead of scan for query plan\n",
    "- statistic includes dist of the value in table/indexed view col\n",
    "- query optimizer determine update automatically, but it can also be done manually, especially if:\n",
    "    - query execution time is slow\n",
    "    - inserting on ascending/descending key col (e.g.identity/timestamp)\n",
    "    - after maintainence operation e.g. bulk insert\n",
    "- not needed to update after rebuild/reorg as rows are not changed\n",
    "- EXEC sp_updatestats  \n",
    "    - Update a particular table or indexed view \n",
    "-   UPDATE STATISTICS [SalesLT].[Address] [AK_Address_rowguid] \n",
    "    WITH FULLSCAN/ SAMPLE 10 PERCENT(, PERSIST_SAMPLE_PERCENT = ON) / RESAMPLE (, PERSIST_SAMPLE_PERCENT = ON) \n",
    "    - with full scan scan all rows\n",
    "    - SAMPLE 10 PERCENT scan a sample\n",
    "    - RESAMPLE most recent sample rate\n",
    "    - PERSIST_SAMPLE_PERCENT set the value as default\n",
    "\n",
    "### db auto tuning\n",
    "- it's a process of learning about workload and identidy issue and improvement using: learn/adapt/verify/repeat\n",
    "- can be configure in intellegince insight in azure portal\n",
    "- for db the below 3 options are available, for mi only force plan, these setting could be inherited from server, multiple on possible\n",
    "    1. force plan=force_last_good_plan, enabled by default, if estimated gain above 10s or error of new plan is above tolerence then last good plan will be used\n",
    "    2. create index: auto create when cpu data io & log io are below 80 %, if performance are'nt improved, it'll be auto drop\n",
    "    3. drop index: not compatiable with partition switch & index hint\n",
    "-   SELECT * FROM sys.indexes  \n",
    "    WHERE auto_created = 1 \n",
    "    - show all index in db that's auto created\n",
    " \n",
    " -  ALTER DATABASE [DP300] SET AUTOMATIC_TUNING = AUTO --| INHERIT | CUSTOM \n",
    "    ALTER DATABASE [DP300] SET AUTOMATIC_TUNING (FORCE_LAST_GOOD_PLAN = ON, \n",
    "                                                CREATE_INDEX = ON, DROP_INDEX = OFF) \n",
    "    - Change database auto-tuning using tsql\n",
    "- SELECT * FROM sys.dm_db_tuning_recommendations \n",
    "\n",
    "\n",
    "### manage storage capcity for azure sql db (may or may not applied to mi)\n",
    "- data space used increase with insert, decrease with delete\n",
    "- data space allocated is space made available for data, increase with data space used , but not decrease with delete\n",
    "- SELECT database_name, allocated_storage_in_megabytes FROM sys.resource_stats \n",
    "    - to check allocated space for a saingle db in masterdb\n",
    "- SELECT elastic_pool_name, elastic_pool_storage_limit_mb, avg_allocated_storage_percent \n",
    "FROM sys.elastic_pool_resource_stats \n",
    "    - to check allocated space for an elastic pool in masterdb\n",
    "- SELECT DATABASEPROPERTYEX('DP300', 'MaxSizeInBytes') \n",
    "    - Display maximum size \n",
    "\n",
    "\n",
    "-   SELECT file_id, type_desc, size, max_size, growth \n",
    "    FROM sys.database_files \n",
    "    WHERE type = 1 \n",
    "    - View current log size \n",
    "    - if size = 1000, 1000 refers to page that are still free, for each page there are 8 kilobytes, so the total available space is 8000 kilobytes\n",
    "-   DBCC SHRINKFILE(file_id, megabyte_to_shrink_to) \n",
    "    DBCC SHRINKDATABASE(DP300) \n",
    "    - To shrink a transaction log file \n",
    "    - file_id megabyte_to_shrink_to are int\n",
    "    - dbcc = database console command\n",
    "- shrinking shouldn't be done too often as file needs to grow(?)\n",
    "\n",
    "### assess growth/fragmentation of db/log\n",
    "-   SELECT database_name, start_time, storage_in_megabytes \n",
    "\n",
    "    FROM sys.resource_stats  \n",
    "\n",
    "    ORDER BY database_name, start_time\n",
    "    -  Assess growth in a database for 1 database (need to be in the \"Master\" database)\n",
    "\n",
    "-   SELECT start_time, elastic_pool_name, elastic_pool_storage_limit_mb, \n",
    "    \n",
    "    avg_allocated_storage_percent  \n",
    "    \n",
    "    FROM sys.elastic_pool_resource_stats \n",
    "    \n",
    "    ORDER BY start_time  \n",
    "\n",
    "    - For an elastic pool (need to be in the \"Master\" database) \n",
    "\n",
    "- EXEC sp_spaceused \n",
    "    - Report on database free space \n",
    "\n",
    "-   SELECT allocated_extent_page_count, unallocated_extent_page_count  \n",
    "\n",
    "    FROM sys.dm_db_file_space_usage \n",
    "\n",
    "    - View number of pages \n",
    "- DBCC SQLPERF (LOGSPACE) \n",
    "    - transaction log space statistic\n",
    "\n",
    "- for vm in ssms disc usage can be directly viewed\n",
    "- SELECT * FROM sys.dm_db_session_space_usage \n",
    "    - show # of pages for each session, internal means tempdb\n",
    "\n",
    "- SELECT * FROM sys.dm_db_task_space_usage \n",
    "    - show # of pages for each task, internal means tempdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd421e0b",
   "metadata": {},
   "source": [
    "# lesson 14: identify performance related issue\n",
    "\n",
    "### query store\n",
    "- query that are regressed due to change in execution plan could be fixed\n",
    "- it's disabled by default on new sql db, on-perm , vm, enabled by defualt on azure sql db\n",
    "- statistic collection interval can be changed, first time activation might takes day until insight coulld be produced, too often collection might affect performance\n",
    "- default allocation for query store is 100 mb - 1 gb depend on sql server version\n",
    "- if limit being hit, query store will not collect new data and turn into read only mode\n",
    "- clean up can be automatically start when storage reach a certain percentage, it'll remove oldest/least expensive query data util back to another certain percentage\n",
    "- query store capture mode, default is auto (ignore infrequent/small/quick executed query), but can be changed to all or none\n",
    "- ALTER DATABASE [DP300] SET QUERY_STORE CLEAR\n",
    "    - remove query store, can also be done in ui in ssms\n",
    "- SELECT * FROM sys.query_store_plan  \n",
    "    - extract query plan\n",
    "\n",
    "-    SELECT Txt.query_text_id, Txt.query_sql_text, Qry.* \n",
    "\n",
    "    FROM sys.query_store_query AS Qry \n",
    "\n",
    "    INNER JOIN sys.query_store_query_text AS Txt \n",
    "\n",
    "    ON Qry.query_text_id = Txt.query_text_id \n",
    "\n",
    "    -  See the queries\n",
    "\n",
    "\n",
    "1. plan store\n",
    "    - for executing plan data\n",
    "\n",
    "2. runtime store\n",
    "    - for execution statistic data\n",
    "3. waits stats store\n",
    "\n",
    "- in ssms you could view regress query and seeduration, cpu time, logical read, physical read etc, in this view you could also set a particular plan for a specific query in future\n",
    "- in ssms you could view query with top overall resource consumption, the view here could be customized whihc stats to see and how far back it should be\n",
    "- in ssms track query you could give the query id and see and see the stats, in the view circle means completed, square means cancelled by client, triangle means failed by exception\n",
    "\n",
    "### identify session causing blocking\n",
    "- if an update is running and the the power is down halfway through, after power back on the computer will rollback the change, there is no half-complete for transaction, only success/fail, this is a lock\n",
    "- explicit transaction: when tsql state begin transaction, comit/rollback transaction, before end the session, the resource is locked\n",
    "- implicit transaction: skip above only write etl\n",
    "- blocking happen when a session wants to work on a resource that is accessing by other session, e.g. 2 updates on the same table at the same time\n",
    "- blocking is caused by poor transactional design/ long running transaction\n",
    "- lock type:\n",
    "    1. row lock: lock at individual row level\n",
    "    2. page lock: lock at page level (a page is 8192 characters)\n",
    "    3. entire table lock:\n",
    "- SELECT * FROM sys.dm_tran_locks \n",
    "    - To view locks: request mode show what the second accessor wants to do\n",
    "        - S: select\n",
    "        - X: exclusive \n",
    "        - IX: intent exclusive\n",
    "        - u: update\n",
    "-   SELECT session_id, blocking_session_id, start_time, status, command, DB_NAME(database_id) as [database], wait_type, wait_resource, wait_time, open_transaction_count \n",
    "\n",
    "    FROM sys.dm_exec_requests \n",
    "\n",
    "    WHERE blocking_session_id > 0; \n",
    "    - To view blocking: \n",
    "\n",
    "\n",
    "### isolation level\n",
    "-   SET TRANSACTION ISOLATION LEVEL  \n",
    "\n",
    "    READ UNCOMMITTED/READ COMMITTED/REPEATABLE READ/SNAPSHOT/SERIALIZABLE \n",
    "    - READ UNCOMMITTED: completely ignore blocking, might cause dirty read(reading data that is not commited and could be rolled back)\n",
    "    - READ COMMITTED:block depend on READ_COMMITTED_SNAPSHOT setting, can only read commited data\n",
    "    - REPEATABLE READ:read remains the same until the end of the same transaction, no block, if:\n",
    "        1. DB is not in recovery stats (restoring from backup)\n",
    "        2. if having another alter db: ALTER DATABASE [DP300] SET ALLOW_SNAPSHOT_ISOLATION ON \n",
    "        - if insert has been done,  REPEATABLE return new data\n",
    "\n",
    "    - SNAPSHOT: read remains the same until the end of the same transaction, no block\n",
    "        - if insert has been done,  SNAPSHOT doesn't return new data,  only expanded version would be generated\n",
    "    - SERIALIZABLE:block any dirty read, can't read stuff that is modified\n",
    "         - block update and insert but not read\n",
    "         - strictest, used in bank\n",
    "\n",
    "\n",
    "\n",
    "- ALTER DATABASE [DP300] SET READ_COMMITTED_SNAPSHOT ON \n",
    "    - off is default for vm, on perm, when off read_commited is blocked\n",
    "    - on is default for azure sql db, DML statement start generating row version that doesn't block read_commited \n",
    "- DBCC USEROPTIONS \n",
    "    - to see the current option\n",
    "\n",
    "| Isolation level   | Dirty read | nonrepeatable read | phantom|\n",
    "| :---------------- | :--------: |  :---------------: |:-----: |\n",
    "| read uncommitted  |      v     |          v         |   v    |\n",
    "| read committed    |            |          v         |   v    |\n",
    "| repeated read     |            |                    |   v    |\n",
    "| snapshot          |            |                    |        |\n",
    "| serealizable      |            |                    |        |\n",
    "\n",
    "- A phantom read occurs when a transaction performs a query with a WHERE clause (e.g., SELECT * FROM orders WHERE value > 1000), and then a second transaction inserts new rows that match the WHERE clause and commits. If the first transaction re-executes the same query, it might see more rows than it did initially. The original rows it read are still the same (repeatable read), but the set of rows has changed\n",
    "    - a transaction end when it's explictly commited/rolled back/ client connection closed etc.\n",
    "- Non-Repeatable Read: existing row always return same value\n",
    "\n",
    "- Phantom Read: new rows might come in\n",
    "\n",
    "### performance related db configuration\n",
    "- this could be done in UI of ssms or tsql\n",
    "- auto close: close when there is no connection , can't be enable in azure sql db\n",
    "- auto_create_statistic: default on, allow generation info of content of each column, good for optimizer to decide for scan or seek\n",
    "    -ALTER DATABASE [DP300]  SET AUTO_CREATE_STATISTICS OFF GO \n",
    "- auto_create_incremental_statistic: default off, collect even more thorough data than auto_create_statistic\n",
    "- auto_shrink: default off, less burden to performance compared to auto_create_statistic but also less effective, not recommanded\n",
    "\n",
    "- ``` \n",
    "    ALTER DATABASE SCOPED CONFIGURATION  \n",
    "    -- [FOR SECONDARY] \n",
    "    SET GLOBAL_TEMPORARY_TABLE_AUTO_DROP = ON \n",
    "    -- LAST_QUERY_PLAN_STATS \n",
    "    -- LEGACY_CARDINALITY_ESTIMATION \n",
    "    -- MAXDOP \n",
    "    -- OPTIMIZE_FOR_AD_HOC_WORKLOADS \n",
    "    -- PARAMETER_SNIFFING \n",
    "    -- QUERY_OPTIMIZER_HOTFIXES \n",
    "    GO \n",
    "    ```\n",
    "    - [FOR SECONDARY] if there is a geo-replicated db\n",
    "    - GLOBAL_TEMPORARY_TABLE_AUTO_DROP drop GLOBAL_TEMPORARY_TABLE if not used in any session\n",
    "    - LAST_QUERY_PLAN_STATS actual execution plan being shown or not in sys.db_exec_PLAN_STATS\n",
    "    - LEGACY_CARDINALITY_ESTIMATION: how many row each value has in db, should only be used for compatibility purpose\n",
    "    - MAXDOP: inter-query parallelism, max # of parellel thread, too high of the number migh impact performance, default 8 is usually a suitable #\n",
    "    - OPTIMIZE_FOR_AD_HOC_WORKLOADS: do a compiled plan stub when a batch is compliled for first time, by the second time, only when compile second time a full compiled plan will be used, this reduce memory footprint\n",
    "    - PARAMETER_SNIFFING : evluate stored procedure to create an execution plan, stick to the plan even parameter of stored procedure change, performance might be suboptimal\n",
    "    - QUERY_OPTIMIZER_HOTFIXES: use the QUERY_OPTIMIZER_HOTFIXES ignoring compatibility\n",
    "\n",
    "### Configure Intelligent Query Processing (IQP) \n",
    "\n",
    "- IQP is a set of features designed to improve query performance\n",
    "- can be done using tsql or ssms ui\n",
    "- first iop were included in sql server 2017 ( interleaved execution tvf & batch mode memory grant feedback), 2019 added a lot more\n",
    " \n",
    "- SELECT * FROM sys.configurations \n",
    "    - Server-wide configuration options \n",
    "- EXEC sp_configure '101', 0\n",
    "    - changing server configuration in azure sql mi or vm, not in db\n",
    "- SELECT * FROM sys.database_scoped_configurations \n",
    "    - Database-wide configuration options\n",
    "- ALTER DATABASE SCOPED CONFIGURATION  SET [config_name] = ON \n",
    "    - changing db configuration \n",
    "    - approx_count_distinct can't be disabled\n",
    "- SELECT * FROM [schema].[table] OPTION (USE HINT ('config_name')) \n",
    "    - changing db configuration for one particular query\n",
    "\n",
    "\n",
    "- adaptive join (batch mode)\n",
    "    - it select between join or the nested looped join during runtime based on scanning actual first input rows\n",
    "    - requirement:\n",
    "        - col stored index in the query\n",
    "        - or a table being referenced in the join\n",
    "        - or batch mode enabled for row stored\n",
    "- approx_count_distinct\n",
    "    - approximation for aggragation of count distinct in each column, suitable for big data, decrease memory & performance requirement, 2 % error rate with 97% possibility\n",
    "    - can't be disabled\n",
    "- batch mode on row store\n",
    "    - queries can work on batches of rows instead of 1 row at a time when cached, reduce db workload\n",
    "    - happens by query plan decision, no need to set up\n",
    "- interleaved execution tvf\n",
    "    - it use the cardinality of multistatement table-valued-function (tvf) on first compilation instead of guessing, only for read only, e.g. select \n",
    "    - default fixed guess is 100 row\n",
    "- batch mode memory grant feedback\n",
    "    - there is also a row mode memory grant feedback\n",
    "    - sql server look for how much memory allocated to a cached query, for the second run allocated same amount of memory to it, alternative is guessing memor, then adjust memory grant next time based on usage\n",
    "- tsql scalar udf lining\n",
    "    - user defined function, usually performa badly bcs they might run multiple time once per row bcs unable to work out performance cost\n",
    "    - scalar udf transform into the equivalent related expression, it takes out a function put it into select (inlined) and increase performance\n",
    "    - doesn't work with all function, especially for function with multiple return \n",
    "    - can be disabled for specific udf\n",
    "- deffered compilation tv\n",
    "    - it use the cardinality of multistatement table variable (tv) on first compilation instead of guessing, only for read only, e.g. select \n",
    "    - default fixed guess is 1 row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0d982",
   "metadata": {},
   "source": [
    "# lesson 15: automatic task and performance backup\n",
    "\n",
    "### elastic job agent\n",
    "- automate db maintenance task\n",
    "- one could have a lot of db all over the place, some are azure sql db and some in the pool, with different server and subscription, for united actions on all these db (target group/ db of schard maps) e.g. backup, credential, collect performance data, telemetry data ( the automatic collection and transmission of data from remote sources), update reference data can be done through elastic job agent, as long as they are in the same azure cloud \n",
    "    - shard maps = how data is distributed across multiple databases (shards)\n",
    "- in mi the equivalent is sql server agent job\n",
    "- it charge for as a azure sql db, must be above basic tier, s0 can but isn't recommended, for frequent job even higher tier would be better\n",
    "- an elastic job db is needed for an agent, a blank or azure sql db could be used to store logs, meta data , job related data, stored procedure \n",
    "- elastic job can execute (regardless of db is created before job agent):\n",
    "    - against all db in a server\n",
    "    - all db in a pool\n",
    "    - single db\n",
    "    - exclude specific db in target\n",
    "- master db credential is needed to enumerate all db\n",
    "    -  enumerate = the process of sequentially accessing each database in a group\n",
    "- a job is a until of work which contains steps, each are tsql scripts and other details\n",
    "- script must be Idempotent \n",
    "    - idempotent = operations produce the same result even when the operation is repeated many times\n",
    "- then it'll generate output and job history, output could be saved as table, job history will be stored in job.job_executions for 45 days\n",
    "- creation step:\n",
    "    1. create azure sql db. minimum s0\n",
    "    2. go to azure portal using ui/powershell create elastic job agent\n",
    "    3. create db master key using tsql in ssms, password must be strong\n",
    "        - CREATE MASTER KEY ENCRYPTION BY PASSWORD='<an6?%9++Vyd%Ut9';\n",
    "    4. CREATE DATABASE SCOPED CREDENTIAL, need one to execute job,  one to refresh db metadata in the server\n",
    "        - CREATE DATABASE SCOPED CREDENTIAL [MasterCred] WITH IDENTITY = 'MasterU', SECRET = 'an6?%9++Vyd%Ut9'\n",
    "        - CREATE DATABASE SCOPED CREDENTIAL [RunJob] WITH IDENTITY = 'JobU', SECRET = 'an6?%9++Vyd%Ut9'\n",
    "    5. Create a target group \n",
    "        - EXEC jobs.sp_add_target_group 'GrpDatabase';\n",
    "    6. add a target group member \n",
    "\n",
    "        Â´Â´Â´\n",
    "        EXEC jobs.sp_add_target_group_member \n",
    "        @target_group_name = 'GrpDatabase', \n",
    "        @target_type = 'SqlDatabase',   -- or 'SqlServer' or 'PoolGroup' \n",
    "        -- @membership_type = 'Exclude', \n",
    "        -- @refresh_credential_name = 'RefreshPassword', \n",
    "        @server_name = 'dp300database.database.windows.net', \n",
    "        @database_name = 'dp300'; \n",
    "        Â´Â´Â´\n",
    "        - @membership_type = 'Exclude'  to exclude a certain computer\n",
    "        - @refresh_credential_name = 'RefreshPassword' when target_type is a server or pool\n",
    "    7. to create master user run this tsql In Master Database \n",
    "        Â´Â´Â´\n",
    "        CREATE LOGIN MasterU WITH PASSWORD ='<an6?%9++Vyd%Ut9' \n",
    "        CREATE USER MasterU FROM LOGIN MasterU \n",
    "        CREATE LOGIN JobU WITH PASSWORD = '<an6?%9++Vyd%Ut9' \n",
    "        Â´Â´Â´\n",
    "        - for the job to run we need 2 users, both with login in master db, one user in master db, one user in Target User db \n",
    "    8. to create user in Target User Database \n",
    "        Â´Â´Â´\n",
    "        CREATE USER JobU FROM LOGIN JobU \n",
    "        ALTER ROLE db_owner ADD MEMBER JobU\n",
    "        Â´Â´Â´\n",
    "    9. Create a job and job steps in elastic job agent db\n",
    "        Â´Â´Â´\n",
    "        EXEC jobs.sp_add_job @job_name='My first job', \n",
    "        @description='Look at objects' \n",
    "        EXEC jobs.sp_add_jobstep @job_name='My first job', \n",
    "        @command='SELECT * FROM sys.objects', \n",
    "        @credential_name='RunJob', \n",
    "        @target_group_name='GrpDatabase' \n",
    "        Â´Â´Â´\n",
    "    10. Run/schedule the job in T-SQL \n",
    "        - EXEC jobs.sp_start_job 'My first job' \n",
    "            - Run the job now\n",
    "\n",
    "        Â´Â´Â´\n",
    "        EXEC jobs.sp_update_job \n",
    "        @job_name='My first job', \n",
    "        @enabled=1, \n",
    "        @schedule_interval_type='Minutes', -- Or Hours, Days, Weeks, Months or Once, \n",
    "        @schedule_interval_count=1 \n",
    "        Â´Â´Â´\n",
    "            - schedule the job \n",
    "    11. Monitor job execution \n",
    "        - SELECT * FROM jobs.job_executions order by start_time \n",
    "        - or in azure portal seeing the last 100 job execution\n",
    "    - adding target groups/credentials can only be done through tsql/powershell/visual studio, but not in azure portal\n",
    "\n",
    "### log backup\n",
    "\n",
    "- in azure sql db backup is done automatically\n",
    "1. full backup \n",
    "    - is done every single week, same as in mi\n",
    "2. differential backup\n",
    "    - done every 12-24 hr, back up the current difference with last full backup\n",
    "3. transactional backup\n",
    "    - every 5 to 10 min backup everything since last backup(all 3 types)\n",
    "- restore to point in time in last 7 day (option 1 to 35, default 7, basic only 7) with these backups is possible\n",
    "- to restore to a point in time last full back up, last differential back up between that full backup and now, all transactional backup between that differential backup and now will be needed\n",
    "\n",
    "- azure portal and powershell can both carry out a recover\n",
    "- a new db is needed for restoration, restore over an existing db is not allowed\n",
    "- restore db usually need to be on the same server\n",
    "- to restore it to a different region, create a new db in the new region, in advanced using backup to populate the db\n",
    "\n",
    "### longterm backup retention (ltr)\n",
    "- available for azure sql mi and db\n",
    "- the functionality is in public preview\n",
    "- a backup for longer than 35 days\n",
    "- only control by azure not admin\n",
    "- might take up to 7 days before the first ltr backup\n",
    "- secondary db should have ltr configured, otherwise failed over from primary db will lead to lose of ltr\n",
    "- backups are stored in azure blob storage, a different storage container each week\n",
    "- in configuration backup length for weekly, monthly, and yearly can be selected, but not timing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e15c3",
   "metadata": {},
   "source": [
    "# lesson 16: high availability and disaster recovery (HADR) strategy for data platform\n",
    "\n",
    "### service level agreement\n",
    "- service level agreement (SLA) is what ms minimum guarantee, if ms failed to meet it, they'll refund\n",
    "1. db\n",
    "    - for hyperscale there is a min uptime for 99,9 % if no replica db, 99,95 % if 1 replica\n",
    "    - other tiers  are 99.99 %replica\n",
    "    - if configured for zone redundant deployment(db that have multiple synchronized in diff building of same region) + business critical/premium then 99,995%\n",
    "2. mi\n",
    "    - 99.99% (four 9)\n",
    "3. vm\n",
    "    - 95% - 99,99%\n",
    "    - sql server on vm might fail on a healthy vm, so real sla is even lower for vm\n",
    "\n",
    "### HADR strategy based on RPO/RTO\n",
    "- geo-replica\n",
    "    - setting can be configured under replica \n",
    "    - primary db send out data through asynchronous replication to replicas/secondaries, the process is called seeding. the update is replicated asynchronously, which means they are committed to primary before to secondary\n",
    "    - secondary need to be at least same service tier as primary, while upgrading, upgrade secondary first then primary, only when changing from general to business critical need to disconnect the 2\n",
    "    - up tp 4 replicas possible\n",
    "    - building replica for replica is possible\n",
    "    - secondary can be read, so using primary for write and secondary for read can help sharing the workload\n",
    "    - after failure, one of the secondary can be appointed as new primary and the other secondary will be connected to this new primary, but the connection string will need to be updated manually\n",
    "- failover group\n",
    "    - with failover group, azure will manage the connection string for a group of db, once primary is failed, it switch to a secondary without changing the string\n",
    "    - if choosing automatic failover policy, manual failover will still be available\n",
    "|                                    | Geo replica | failover group |\n",
    "| :--------------------------------: | :-: | :-: |\n",
    "| automatic failover                 |  x  |  v  |\n",
    "| failover multiple db simultaneously|  x  |  v  |\n",
    "| sql mi support                     |  x  |  v  |\n",
    "| user update connection string after failover (time consuming) |  v   | x |\n",
    "| can be in same region as primary   |  v  |  x  |\n",
    "| multiple replica                   |  v  |  x  |\n",
    "| support read-scale                 |  v  |  v  |\n",
    "\n",
    "- recovery point objective (RPO) = how much data one could lose \n",
    "- recovery time objective (RTO) = max failover time, during rto no backup will happen\n",
    "- with a geo replication,\n",
    "    - manual db failover, rpo is 5 sec, rto is 30 sec\n",
    "    - auto failover group, rpo is 5 sec, rto is 1 hr minimum\n",
    "- for a geo-restore from geo replicated backup, rpo is 1 hr, rto is 12 hr\n",
    "- the shorter the fail timeframe, the more expensive it cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb4d83",
   "metadata": {},
   "source": [
    "# lesson 17 perform administrative using tsql\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798db6e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
